{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQN9HRq1ACL6"
      },
      "source": [
        "# ECE 157B/272B Homework 1B Code\n",
        "Created by: Min Jian Yang and Matthew Dupree\\\n",
        "Parts of the code are from:\n",
        "- The PyTorch tutorial: https://pytorch.org/tutorials/beginner/transformer_tutorial.html#load-and-batch-data\n",
        "- The KDnuggets tutorial: https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUCN8AxsACL7"
      },
      "source": [
        "## Select a GPU Runtime\n",
        "* If you would like to complete this assignment on your own computer, make sure that your computer has a GPU compatible with PyTorch hardware acceleration. Without a GPU, training the models will take a very long time.\n",
        "* If you are using Google Colab, you will want to limit the amount of time you spend using a GPU runtime, to prevent running up against usage limits. If you are planning to train models this session, (i.e. beyond just vocabulary exploration,) you will need to select a GPU runtime by going to `Runtime` > `Change runtime type` > `Hardware accelerator` > `GPU`.\n",
        "    * For each model you intend to train and evaluate, you will need 10-15 minutes of GPU runtime, so plan in advance.\n",
        "    * If your computer goes to sleep or loses its internet connection for more than a minute or two, your Google Colab runtime will be recycled, erasing your model and training progress.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlbShgiSACL8"
      },
      "source": [
        "## Install Libraries\n",
        "The below lab will check the versions of the PyTorch and Pandas libraries you have installed. We're expecting to see these versions of these packages, but later versions may also work fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svpx-oi_ACL8"
      },
      "outputs": [],
      "source": [
        "%pip install \"torch>=2.1.0\" \"tokenizers>=0.15.0\" \"matplotlib\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4fIz5suACL9"
      },
      "source": [
        "You may be asked to restart the kernel after running the above cell. If so, please do so and then continue running the cells below. (This means specifically to \"Restart\" the _kernel_, not to get a new runtime, as a new runtime will not have the package versions installed.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4GnolNrACL9"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIfCPFF0ACL9"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List, Callable, Tuple, NamedTuple, Union, Optional\n",
        "from collections import Counter, defaultdict\n",
        "from pathlib import Path\n",
        "\n",
        "import math\n",
        "import string\n",
        "import statistics\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import Tensor, jit\n",
        "\n",
        "import torch.nn\n",
        "from torch.nn import Module\n",
        "from torch.nn import RNN, LSTM, GRU, Embedding, Linear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0tPFWylACL9"
      },
      "source": [
        "## Unpack Data\n",
        "\n",
        "If you are using Google Colab, you will need to upload the `data.zip` file from the assignment folder to your Colab runtime using the file browser on the left side of the screen. Also upload the `tokenizer_toy.py` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTWJTjKwACL9"
      },
      "outputs": [],
      "source": [
        "def please_upload(filename: str) -> Path:\n",
        "    path = Path(filename)\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"Please upload the {path.name} file from the assignment to: {Path.cwd()/path}\"\n",
        "        )\n",
        "    return path\n",
        "\n",
        "\n",
        "DATA_PATH = Path(\"data\")\n",
        "if DATA_PATH.exists() and not DATA_PATH.is_dir():\n",
        "    raise NotADirectoryError(\n",
        "        f\"Please remove the file {DATA_PATH} from the directory and try again.\"\n",
        "    )\n",
        "if not DATA_PATH.exists():\n",
        "    print(\"Data directory not found, creating it...\")\n",
        "\n",
        "    DATA_ZIP = please_upload(\"data.zip\")\n",
        "\n",
        "    print(\"Found {DATA_ZIP}, extracting it to {DATA_PATH}...\")\n",
        "\n",
        "    import zipfile\n",
        "\n",
        "    with zipfile.ZipFile(DATA_ZIP, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(DATA_PATH)\n",
        "    print(\"Done!\")\n",
        "\n",
        "# In case the data gets nested in an extra directory\n",
        "if (DATA_PATH/\"data\").is_dir():\n",
        "    print(\"Data directory is nested in an extra directory, unnesting it...\")\n",
        "    data2 = DATA_PATH.parent/\"data.tmp\"\n",
        "    (DATA_PATH/\"data\").rename(data2)\n",
        "    DATA_PATH.rmdir()\n",
        "    data2.rename(DATA_PATH)\n",
        "assert DATA_PATH.is_dir() and any(f.is_file() for f in DATA_PATH.iterdir()), \"No data found in {DATA_PATH}! Something went wrong. Try deleting the \\\"data\\\" directory and running this cell again.\"\n",
        "\n",
        "\n",
        "TOKENIZER_TOY_PATH = please_upload(\"tokenizer_toy.py\")\n",
        "\n",
        "print(\"Data directory and tokenizer_toy.py ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q4YV2ngACL9"
      },
      "source": [
        "### Import the tokenizer\n",
        "This assignment, we've split the tokenizer into a separate file, as we'd like to keep it constant across all models. This will allow us to compare the models' vocabularies and predictions more easily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxviIoU3ACL-"
      },
      "outputs": [],
      "source": [
        "import tokenizer_toy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlKUdSH4ACL-"
      },
      "source": [
        "### Play with the tokenizer\n",
        "\n",
        "The below two cells will load and train a tokenizer on the training data, allowing you to play with the tokenizer's behavior. Uncomment the cells to make use of them. (They are commented by default to allow you to run all cells without interruption.)\n",
        "\n",
        "If you are using VSCode, the interactive behavior of the tokenizer may not work. If so, interrupt the cell and run the command after the `!` in a terminal set to the same environment and directory as this notebook. (You can use VSCode's built-in terminal for this.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHAEo9uyACL-"
      },
      "outputs": [],
      "source": [
        "# !python tokenizer_toy.py data/daily_dialog_train_text.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6kHiK1IACL-"
      },
      "outputs": [],
      "source": [
        "# !python tokenizer_toy.py data/shakespeare_train.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j421NaBzACL-"
      },
      "source": [
        "## Use GPU if available\n",
        "The below line checks if a CUDA environment (typically a GPU accelerator) is available. If so, we'll use it, otherwise we'll use the CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieEVHcD1ACL_"
      },
      "outputs": [],
      "source": [
        "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: {DEVICE}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvI4m1ahACL_"
      },
      "source": [
        "## Prepare the dataset for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nij8yB1qACL_"
      },
      "source": [
        "### Prepare a tokenization function\n",
        "\n",
        "This function will take a string and return a PyTorch tensor of token IDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em-kvDzPACL_"
      },
      "outputs": [],
      "source": [
        "def tokenize_dataset(\n",
        "    dataset: str,\n",
        "    tokenizer: tokenizer_toy.SimpleWordPieceTokenizer\n",
        ") -> torch.Tensor:\n",
        "    return torch.tensor(\n",
        "        tokenizer.encode(dataset).ids,\n",
        "        dtype=torch.long,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8vHFW4nACL_"
      },
      "source": [
        "### Load the datasets\n",
        "\n",
        "As both datasets have been separated into train and validation sets for you, we can greatly simplify the dataset loading process from the previous assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CINEyoUhACL_"
      },
      "outputs": [],
      "source": [
        "def load_dataset(\n",
        "    filename: Union[str, Path],\n",
        ") -> str:\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "        result = f.read()\n",
        "    print(f\"Loaded {len(result)} chars from {filename}\")\n",
        "    return result\n",
        "\n",
        "shakespeare_train_text = load_dataset(\"data/shakespeare_train.txt\")\n",
        "shakespeare_val_text = load_dataset(\"data/shakespeare_val.txt\")\n",
        "daily_dialog_train_text = load_dataset(\"data/daily_dialog_train_text.txt\")\n",
        "daily_dialog_val_text = load_dataset(\"data/daily_dialog_val_text.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdqgELRHACMA"
      },
      "source": [
        "### Build vocabularies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZqGc922ACMA"
      },
      "outputs": [],
      "source": [
        "shakespeare_only_tokenizer = tokenizer_toy.SimpleWordPieceTokenizer(\n",
        "    training_files=[\"data/shakespeare_train.txt\"]\n",
        ")\n",
        "print(\"Shakespeare tokenizer vocab size:\", shakespeare_only_tokenizer.get_vocab_size())\n",
        "\n",
        "daily_dialog_only_tokenizer = tokenizer_toy.SimpleWordPieceTokenizer(\n",
        "    training_files=[\"data/daily_dialog_train_text.txt\"]\n",
        ")\n",
        "print(\"Daily Dialog tokenizer vocab size:\", daily_dialog_only_tokenizer.get_vocab_size())\n",
        "\n",
        "both_tokenizer = tokenizer_toy.SimpleWordPieceTokenizer(\n",
        "    training_files=[\"data/shakespeare_train.txt\", \"data/daily_dialog_train_text.txt\"]\n",
        ")\n",
        "print(\"Combined tokenizer vocab size:\", both_tokenizer.get_vocab_size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcSMJwviACMA"
      },
      "source": [
        "### Tokenize the datasets\n",
        "\n",
        "Because we have three different tokenizers and two different datasets, we'll need to tokenize each dataset with each tokenizer. We'll do this in a nested loop, so that we can keep track of which tokenizer was used for each dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylB2ZjH0ACMA"
      },
      "outputs": [],
      "source": [
        "tokenizers = {\n",
        "    'shakespeare_tok': shakespeare_only_tokenizer,\n",
        "    'daily_dialog_tok': daily_dialog_only_tokenizer,\n",
        "    'both_tok': both_tokenizer\n",
        "}\n",
        "datasets = {\n",
        "    'shakespeare_train_text': shakespeare_train_text,\n",
        "    'daily_dialog_train_text': daily_dialog_train_text,\n",
        "}\n",
        "tokenized_datasets = {}\n",
        "for tok_name, tok in tokenizers.items():\n",
        "    for dataset_name, dataset in datasets.items():\n",
        "        print(f\"Tokenizing {dataset_name} with {tok_name}...\", end=\"\")\n",
        "        result = tokenize_dataset(dataset, tok)\n",
        "        tokenized_datasets[f\"{dataset_name}_{tok_name}\"] = result\n",
        "        print(f\"({len(result)} tokens) Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4k9evjbACMA"
      },
      "source": [
        "### Analyze Tokenizer Differences\n",
        "\n",
        "The tokenizers above differ only by their vocabularies. Below, we'll compute a few metrics to compare the vocabularies' contents.\n",
        "\n",
        "Individual metrics:\n",
        "\n",
        "* Vocabulary Size: How many tokens are in each tokenizer's vocabulary?\n",
        "* Token Types: How many tokens are of each of the following types:\n",
        "    * Alphabetic: Contains only letters\n",
        "    * Numeric: Contains only numbers\n",
        "    * Alphanumeric: Contains both letters and numbers\n",
        "    * Whitespace: Contains only whitespace\n",
        "    * Punctuation: Contains only punctuation\n",
        "    * Subword: Begins with `##`\n",
        "* Token Length Curves: We'll plot the length of each token in the two tokenizers' vocabularies. This will allow us to see how the tokenizers' vocabularies differ in terms of the tokens' lengths -- how many characters each token contains. (Are we learning large words or lots of small, strange combinations of characters?)\n",
        "\n",
        "Dataset-specific metrics:\n",
        "* Encoded Token Counts: Count how many tokens total a tokenizer needs to encode each dataset. This will allow us to see how the tokenizers' vocabularies differ in terms of compressing the data.\n",
        "* Token Frequency Curves: We'll plot the frequency of each token in the two tokenizers' vocabularies. This will allow us to see how the tokenizers' vocabularies differ in terms of the tokens' relative frequencies -- how often each token appears in the datasets they were trained on.\n",
        "\n",
        "Comparison metrics:\n",
        "\n",
        "* Intersection: How many tokens do the two tokenizers have in common?\n",
        "* Union: How many tokens do the two tokenizers have in total between the two?\n",
        "* Jaccard Index: The Jaccard index is a measure of similarity between two sets. It is defined as the size of the intersection divided by the size of the union of the two sets. In this case, we'll use it to measure how similar the two tokenizers' vocabularies are -- how many tokens they have in common out of the total number of tokens they have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6grVQNvACMA"
      },
      "outputs": [],
      "source": [
        "def token_len_operation(\n",
        "    tokens: List[str],\n",
        "    token_lengths: List[int],\n",
        "    operation_name: str,\n",
        "    operation: Callable[[List[int]], int]\n",
        ") -> None:\n",
        "    target_token_length = operation(token_lengths)\n",
        "    target_length_tokens = [t for t in tokens if len(t) == target_token_length]\n",
        "    if len(target_length_tokens) < 5:\n",
        "        print(f\"\\t{operation_name}: {target_token_length} {target_length_tokens}\")\n",
        "    else:\n",
        "        print(f\"\\t{operation_name}: {target_token_length} ({len(target_length_tokens)} tokens)\")\n",
        "\n",
        "for tok_name, tok in tokenizers.items():\n",
        "    # Individual Metrics\n",
        "    print(f\"=== {tok_name} ===\")\n",
        "    vocab: Dict[str, int] = tok.get_vocab()\n",
        "    print(f\"Vocab size: {len(vocab)}\")\n",
        "    print(\"Token Type Counts:\")\n",
        "    print(\"\\tAlphabetic:  \", sum(t.isalpha() for t in vocab))\n",
        "    print(\"\\tNumeric:     \", sum(t.isnumeric() for t in vocab))\n",
        "    print(\"\\tAlphanumeric:\", sum(t.isalnum() for t in vocab))\n",
        "    print(\"\\tWhitespace:  \", sum(t.isspace() for t in vocab))\n",
        "    print(\"\\tPunctuation: \", sum(t in string.punctuation for t in vocab))\n",
        "    print(\"\\tSubword:     \", sum(t.startswith(\"##\") for t in vocab))\n",
        "\n",
        "    print(\"Token length distribution:\")\n",
        "    tokens = list(vocab.keys())\n",
        "    token_lengths = [len(t) for t in tokens]\n",
        "    token_len_operation(tokens, token_lengths, \"Min\", min)\n",
        "    token_len_operation(tokens, token_lengths, \"Max\", max)\n",
        "    print(\"\\tMean:\", statistics.mean(token_lengths))\n",
        "    token_len_operation(tokens, token_lengths, \"Median\", statistics.median_low)\n",
        "    print(\"\\tHistogram:\")\n",
        "    counts = Counter(token_lengths)\n",
        "    plt.figure()\n",
        "    plt.bar(counts.keys(), counts.values())\n",
        "    plt.title(f\"{tok_name} Token Length Distribution\")\n",
        "    plt.xlabel(\"Token Length\")\n",
        "    plt.ylabel(\"Token Count\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQxJQ47ZACMA"
      },
      "outputs": [],
      "source": [
        "# Dataset-specific metrics\n",
        "for tok_name in tokenizers:\n",
        "    for dataset_name in datasets:\n",
        "        print(f\"=== {tok_name} on {dataset_name} ===\")\n",
        "        result = tokenized_datasets[f\"{dataset_name}_{tok_name}\"]\n",
        "        print(f\"Dataset size: {len(result)} tokens\")\n",
        "        print(\"Token frequency distribution:\")\n",
        "        token_counts = torch.bincount(result)\n",
        "        print(\"\\tMin:\", token_counts.min().item())\n",
        "        print(\"\\tMax:\", token_counts.max().item())\n",
        "        print(\"\\tMean:\", token_counts.float().mean().item())\n",
        "        print(\"\\tMedian:\", token_counts.float().median().item())\n",
        "        print(\"\\tHistogram:\")\n",
        "        plt.figure()\n",
        "        plt.hist(token_counts, bins=250, log=True)\n",
        "        plt.title(f\"{tok_name} Token Frequency Distribution\")\n",
        "        plt.xlabel(\"Token Frequency\")\n",
        "        plt.ylabel(\"Token Count\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMiR7AGdACMB"
      },
      "outputs": [],
      "source": [
        "# Comparison metrics\n",
        "for tok_name1, tok in tokenizers.items():\n",
        "    for tok_name2, tok in tokenizers.items():\n",
        "        if tok_name1 <= tok_name2:\n",
        "            continue\n",
        "        print(f\"=== {tok_name1} vs {tok_name2} ===\")\n",
        "        vocab1: Dict[str, int] = tokenizers[tok_name1].get_vocab()\n",
        "        vocab2: Dict[str, int] = tokenizers[tok_name2].get_vocab()\n",
        "        vocab1_set = set(vocab1.keys())\n",
        "        vocab2_set = set(vocab2.keys())\n",
        "        intersection = vocab1_set.intersection(vocab2_set)\n",
        "        union = vocab1_set.union(vocab2_set)\n",
        "        print(f\"\\t{tok_name1} has {len(vocab1_set)} tokens\")\n",
        "        print(f\"\\t{tok_name2} has {len(vocab2_set)} tokens\")\n",
        "        print(f\"\\tIntersection has {len(intersection)} tokens\")\n",
        "        print(f\"\\tUnion has {len(union)} tokens\")\n",
        "        print(f\"\\tJaccard Similarity: {len(intersection) / len(union)}\")\n",
        "        print(f\"\\t{tok_name1}-only tokens: {len(vocab1_set - vocab2_set)} tokens\")\n",
        "        print(f\"\\t{tok_name2}-only tokens: {len(vocab2_set - vocab1_set)} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDDgRPaeACMB"
      },
      "source": [
        "## Finalize Tokenizer and Dataset\n",
        "\n",
        "Now that we've analyzed the tokenizers' tradeoffs, we'll select the `both_tokenizer` as our final tokenizer, as it has the largest vocabulary and high Jaccard index with both of the other tokenizers. Its compression of both datasets is also comparable to the other tokenizers' individual compression of their respective datasets.\n",
        "\n",
        "By saving the Tokenizer, we can load it later without having to retrain it, which will save us time. It also allows us to keep it constant across all models, which will allow us to compare the models' vocabularies and predictions more easily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0Hwk202ACMB"
      },
      "source": [
        "### Save the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqe1GhQRACMB"
      },
      "outputs": [],
      "source": [
        "both_tokenizer.save(\"tokenizer.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ22wbNvACMB"
      },
      "source": [
        "### Re-load the tokenizer\n",
        "For future runs, you can skip here after \"Load the datasets\" when you want to use the same tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FA5C3pymACMB"
      },
      "outputs": [],
      "source": [
        "tokenizer = tokenizer_toy.SimpleWordPieceTokenizer.load(\"tokenizer.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GFzkhxCACMB"
      },
      "source": [
        "### Tokenize the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JbI-_dWACMC"
      },
      "outputs": [],
      "source": [
        "shakespeare_train_tokens = tokenize_dataset(shakespeare_train_text, tokenizer)\n",
        "print(\"shakespeare_train_tokens:\", len(shakespeare_train_tokens), \"tokens\")\n",
        "shakespeare_val_tokens = tokenize_dataset(shakespeare_val_text, tokenizer)\n",
        "print(\"shakespeare_val_tokens:\", len(shakespeare_val_tokens), \"tokens\")\n",
        "daily_dialog_train_tokens = tokenize_dataset(daily_dialog_train_text, tokenizer)\n",
        "print(\"daily_dialog_train_tokens:\", len(daily_dialog_train_tokens), \"tokens\")\n",
        "daily_dialog_val_tokens = tokenize_dataset(daily_dialog_val_text, tokenizer)\n",
        "print(\"daily_dialog_val_tokens:\", len(daily_dialog_val_tokens), \"tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYq0F38pACMC"
      },
      "source": [
        "## Batchify dataset\n",
        "- Create batches out of the long list of number\n",
        "- This allows for parallel computation during training\n",
        "- Drawback being the relationship between the batches are not learned\n",
        "\n",
        "See this link for more detail: https://pytorch.org/tutorials/beginner/transformer_tutorial.html#load-and-batch-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mC6mXIwuACMC"
      },
      "outputs": [],
      "source": [
        "@torch.jit.script\n",
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CcJqGOEACMC"
      },
      "source": [
        "## Generate input and target sequence\n",
        "- Grab an input sequence with length `seq_len` from the batchify data starting at index `i`\n",
        "- The target sequence also has the same length `seq_len` but the starting index is `i+1`\n",
        "\n",
        "See this link for more detail: https://pytorch.org/tutorials/beginner/transformer_tutorial.html#functions-to-generate-input-and-target-sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4UMEie5ACMM"
      },
      "outputs": [],
      "source": [
        "def get_batch(source: Tensor, i: int, bptt: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QVbJl93ACMM"
      },
      "outputs": [],
      "source": [
        "print(\"shakespeare_train_text\", len(batchify(shakespeare_train_tokens, 64)))\n",
        "print(\"daily_dialog_train_text\", len(batchify(daily_dialog_train_tokens, 64)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp9NhqLmACMM"
      },
      "outputs": [],
      "source": [
        "# Example batch drawn from the data and decoded\n",
        "# to show what the model will be predicting against\n",
        "def print_example_batch(tokenizer: tokenizer_toy.SimpleWordPieceTokenizer, training_data: Tensor):\n",
        "    example_batch = batchify(training_data, 2)\n",
        "    example_data, example_targets = get_batch(example_batch, 6485, 55)\n",
        "    print(f\"Example data: {example_data.t()}\")\n",
        "    print(f\"Example targets: {example_targets}\")\n",
        "    print(\n",
        "        f\"=== Example data[0] decoded ===\\n{tokenizer.decode(example_data[:, 0].tolist())}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"=== Example data[1] decoded ===\\n{tokenizer.decode(example_data[:, 1].tolist())}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"=== Example targets decoded ===\\n{tokenizer.decode(example_targets.tolist()[:20])}\"\n",
        "    )\n",
        "\n",
        "\n",
        "print(\"=== Shakespeare ===\")\n",
        "print_example_batch(tokenizer, shakespeare_train_tokens)\n",
        "print(\"=== Daily Dialog ===\")\n",
        "print_example_batch(tokenizer, daily_dialog_train_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uKOU9hbACMM"
      },
      "source": [
        "## Create recurrent neural network model\n",
        "A recurrent neural network with an embedding layer, one or more recurrent layers, and a linear layer.\n",
        "\n",
        "We use this `RecurrentModel` class to describe the behavior of the embedding and output layers, which are the same no matter which recurrent unit we use. We then pass the recurrent unit as a parameter to the `RecurrentModel` constructor to describe the behavior of the recurrent layers. This lets us easily create models with different recurrent units to compare their performance.\n",
        "\n",
        "Once again, this model has not changed from the last assignment. Copy over your `forward` implementation from before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hvxT3tlACMM"
      },
      "outputs": [],
      "source": [
        "class RecurrentModel(Module):\n",
        "    \"\"\"The recurrent neural network.\"\"\"\n",
        "\n",
        "    # __init__ is the constructor.\n",
        "    # It sets up new instances of the class.\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        recurrent_module: Union[RNN, LSTM, GRU],\n",
        "    ) -> None:\n",
        "        \"\"\"Initialize the recurrent neural network.\n",
        "\n",
        "        Note: For this model architecure, if the number of layers is greater\n",
        "        than 1, then embedding size and hidden size must be equal.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): The number of vocabulary in the dataset.\n",
        "            embedding_dim (int): The dimension of the embedding output.\n",
        "            hidden_size (int): The size of the recurrent unit's hidden state.\n",
        "            num_layers (int): The number of recurrent unit layers.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the number of layers is greater than 1 and the\n",
        "            embedding size is not equal to the hidden size.\n",
        "        \"\"\"\n",
        "        # First, we call the superclass constructor\n",
        "        # to give PyTorch a chance to set up the\n",
        "        # parts of the object it needs to.\n",
        "        super().__init__()\n",
        "\n",
        "        # num_layers is how many layers the recurrent unit has.\n",
        "        num_layers = recurrent_module.num_layers\n",
        "\n",
        "        # hidden_size is the size of the hidden state of the recurrent unit.\n",
        "        hidden_size = recurrent_module.hidden_size\n",
        "\n",
        "        # embedding_dim is the size of the embedding output.\n",
        "        embedding_dim = recurrent_module.input_size\n",
        "\n",
        "        if (num_layers > 1) and (hidden_size != embedding_dim):\n",
        "            raise ValueError(\n",
        "                \"When the number of layers is greater than 1, the embedding \"\n",
        "                \"size and hidden size must be equal.\"\n",
        "            )\n",
        "\n",
        "        # The embedding layer turns the token IDs into embedding vectors.\n",
        "        # This is a matrix of size [vocab_size, embedding_dim].\n",
        "        # Each row of the matrix corresponds to one token in the vocabulary,\n",
        "        #  providing a sort of \"meaning in isolation\" for that token.\n",
        "        # The embedding layer is a trainable part of the model,\n",
        "        #  so it will be updated during training.\n",
        "        # The values start out initialized randomly.\n",
        "        self.embedding = Embedding(\n",
        "            num_embeddings=vocab_size, embedding_dim=embedding_dim\n",
        "        )\n",
        "\n",
        "        # The recurrent module is the main part of the model.\n",
        "        # It takes in a sequence of embedding vectors\n",
        "        #  and spreads information across the sequence.\n",
        "        self.rnn = recurrent_module\n",
        "\n",
        "        # The linear layer is the output layer of the model.\n",
        "        # It takes in the final embedding vector from the\n",
        "        #  recurrent module and outputs a vector of size [vocab_size].\n",
        "        # This vector contains a score for each token in the vocabulary.\n",
        "        # The token with the highest score is the one the model predicts\n",
        "        #  as the next token, so the model will try to make the score\n",
        "        #  for the correct token as high as possible.\n",
        "        self.linear = Linear(hidden_size, vocab_size)\n",
        "\n",
        "    # The forward method is called when we pass data through the model.\n",
        "    def forward(\n",
        "        self,\n",
        "        x: Tensor,\n",
        "        prev_state: Optional[Tensor],\n",
        "    ) -> Tuple[Tensor, Tensor]:\n",
        "        \"\"\"Pass the a batch of data through the recurrent neural network model\n",
        "        along with the previous state.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): The batch of data.\n",
        "            prev_states (Tensor): The previous states of the recurrent units.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[Tensor, Tensor]: The logits and hidden states.\n",
        "        \"\"\"\n",
        "\n",
        "        # The embedding layer turns the token IDs into embedding vectors.\n",
        "        # TODO: Embed the input tokens using the embedding layer.\n",
        "\n",
        "        # The recurrent module takes in a sequence of embedding vectors\n",
        "        #  and spreads information across the sequence via its hidden state.\n",
        "        # TODO: Pass the embedded input tokens into the recurrent module along with the previous state.\n",
        "\n",
        "        # The linear layer takes in the final embedding vector from the\n",
        "        #  recurrent module and outputs a vector of size [vocab_size].\n",
        "        # TODO: Pass the final embedding vector into the linear layer to get the logits.\n",
        "\n",
        "        # Return the logits and the final states.\n",
        "        # TODO: Return the logits and the final states.\n",
        "\n",
        "    @torch.jit.export\n",
        "    def detach_state_(self, states: Union[Tensor, Tuple[Tensor, Tensor]]) -> None:\n",
        "        \"\"\"Detach the state of the recurrent units.\n",
        "\n",
        "        This function is used to make sure the model doesn't try to backpropagate\n",
        "        through the entire history of the sequence, as our computers can't\n",
        "        handle that much data.\n",
        "\n",
        "        Args:\n",
        "            states (Union[Tensor, Tuple[Tensor, Tensor]]): The states of the\n",
        "            recurrent units.\n",
        "        \"\"\"\n",
        "        if isinstance(states, Tensor):\n",
        "            states.detach_()\n",
        "        elif isinstance(states, tuple):\n",
        "            for state in states:\n",
        "                state.detach_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgtD0RzmACMN"
      },
      "source": [
        "## Parameters to tune for your experiments\n",
        "The block below defines a \"Parameters\" class that you can use to configure settings for training runs of your various models. We will guide you through creating training functions that will make use of these parameters to allow you to create mostly-reusable code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLKcl-wbACMN"
      },
      "outputs": [],
      "source": [
        "class Parameters(NamedTuple):\n",
        "    ### Dataset and training parameters\n",
        "    # How many different chunks of text to train on at the same time\n",
        "    BATCH_SIZE: int\n",
        "    # Token target -- how many tokens to backpropagate through at most\n",
        "    BPTT: int\n",
        "    # Learning rate\n",
        "    LR: float\n",
        "    # Number of epochs to train for\n",
        "    EPOCHS: int\n",
        "\n",
        "    ### Model parameters\n",
        "    # Number of dimensions in the embedding of each token.\n",
        "    #   More dimensions --> more meaning, but more computation\n",
        "    EMBEDDING_DIM: int\n",
        "    # Number of dimensions in the hidden state in the recurrent model\n",
        "    HIDDEN_DIM: int\n",
        "    # Number of hidden layers in the recurrent model\n",
        "    NUM_LAYERS: int"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1713QkSACMN"
      },
      "source": [
        "## Define training and evaluation functions\n",
        "- `train_epoch` trains the model for one epoch\n",
        "- `evaluate` evaluates the model on the validation set\n",
        "- `train_run` trains the model for all `EPOCHS` epochs and prints as it goes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdvEeun8ACMN"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model: RecurrentModel, vocab_size: int, optimizer: torch.optim.Optimizer, train_data: Tensor, parameters:Parameters) -> Tuple[float,float]:\n",
        "    \"\"\"Training function for our recurrent model.\n",
        "\n",
        "    Args:\n",
        "        model (RecurrentModel): The model to train.\n",
        "        train_data (Tensor): Batchified training data.\n",
        "    \"\"\"\n",
        "    # Turn on training mode which enables training-specific\n",
        "    # layer functionality (e.g. dropout)\n",
        "    model.train()\n",
        "\n",
        "    # Keep track of the loss as we go\n",
        "    total_loss: float = 0.\n",
        "\n",
        "    # Determine how many steps we'll need to cover the whole dataset\n",
        "    num_batches: int = len(train_data) // parameters.BPTT\n",
        "\n",
        "    # Keep track of the hidden state between batches.\n",
        "    state = None\n",
        "\n",
        "    # For each of our batches\n",
        "    for i in range(0, train_data.size(0) - 1, parameters.BPTT):\n",
        "        # Throw away gradients from previous step\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get our input and target batches\n",
        "        data, targets = get_batch(train_data, i, parameters.BPTT)\n",
        "\n",
        "        # Evaluate the model to get logits and the new hidden state\n",
        "        output, state = model(data, state)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = torch.nn.functional.cross_entropy(output.view(-1, vocab_size), targets)\n",
        "\n",
        "        # Make sure the state does not carry gradients\n",
        "        # between batches so we avoid a runaway memory leak\n",
        "        model.detach_state_(state)\n",
        "\n",
        "        # Compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradients to avoid exploding gradients\n",
        "        # (Ask us TAs if you're curious why this is necessary)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "\n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Keep track of the loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Return the average loss across batches\n",
        "    cur_loss = total_loss / num_batches\n",
        "    # and the perplexity\n",
        "    ppl = math.exp(cur_loss)\n",
        "    return cur_loss, ppl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Us6GgWr0ACMN"
      },
      "outputs": [],
      "source": [
        "def evaluate(model: RecurrentModel, vocab_size: int, val_data: Tensor, parameters: Parameters) -> Tuple[float, float]:\n",
        "    \"\"\"Evaluate the perplexity of the model on the validation data.\n",
        "\n",
        "    Args:\n",
        "        model (RecurrentModel): The model to evaluate.\n",
        "        val_data (Tensor): Batchified validation data.\n",
        "\n",
        "    Returns:\n",
        "        float: The perplexity of the model on the validation data.\n",
        "    \"\"\"\n",
        "    # Turn on evaluation mode which disables\n",
        "    # training-specific functionality (e.g. dropout)\n",
        "    model.eval()\n",
        "\n",
        "    # Keep track of the loss as we go\n",
        "    total_loss: float = 0.\n",
        "\n",
        "    # Determine how many steps we'll need to cover the whole dataset\n",
        "    num_batches: int = len(val_data) // parameters.BPTT\n",
        "\n",
        "    # Keep track of the hidden state between batches.\n",
        "    state = None\n",
        "    with torch.no_grad(): # No need to track gradients here, since we're not training\n",
        "        for i in range(0, val_data.size(0) - 1, parameters.BPTT):\n",
        "            # Get our input and target batches\n",
        "            data, targets = get_batch(val_data, i, parameters.BPTT)\n",
        "\n",
        "            # Evaluate the model to get logits and the new hidden state\n",
        "            output, state = model(data, state)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = torch.nn.functional.cross_entropy(output.view(-1, vocab_size), targets)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    # Return the average loss across batches\n",
        "    cur_loss = total_loss / num_batches\n",
        "    # and the perplexity\n",
        "    ppl = math.exp(cur_loss)\n",
        "    return cur_loss, ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaYA2G8xACMO"
      },
      "outputs": [],
      "source": [
        "def train_run(model: RecurrentModel, vocab_size: int, train_data: Tensor, validation_data: Tensor, parameters: Parameters, verbose: bool = True) ->None:\n",
        "    \"\"\"Train a model for NUM_EPOCHS epochs.\n",
        "    \"\"\"\n",
        "    # Here we define an optimizer and scheduler to use for training.\n",
        "    # An optimizer adjusts the parameters of the model based on the loss\n",
        "    # and the gradients of the parameters with respect to the loss.\n",
        "\n",
        "    # Adam is a popular optimizer that tends to work well in practice.\n",
        "    # optimizer = torch.optim.Adam(model.parameters(), lr=parameters.LR) # Set LR to about 0.005 to start\n",
        "\n",
        "    # AdamW corrects a minor flaw in the original Adam implementation\n",
        "    # allowing it to converge slightly better.\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=parameters.LR) # Set LR to about 0.01\n",
        "\n",
        "    # SGD is the simple stochastic gradient descent algorithm,\n",
        "    # implemented as LR * gradient.\n",
        "    # Notably, the RNNs shown here need a strong learning rate to\n",
        "    # even begin to move the loss, so we start with a high LR.\n",
        "    # optimizer = torch.optim.SGD(model.parameters(), lr=parameters.LR) # Set LR to about 10.0 to start\n",
        "\n",
        "    # The scheduler adjusts the learning rate over time.\n",
        "    # Here we use a scheduler that decreases the learning rate\n",
        "    # by a factor of 0.5 if the training loss doesn't decrease\n",
        "    # for two epochs in a row.\n",
        "    # This makes sure that the model can converge down into a\n",
        "    # minimum once it finds one, instead of bouncing around.\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=1, threshold=1e-3)\n",
        "\n",
        "    # Train for NUM_EPOCHS epochs\n",
        "    for epoch in range(1,parameters.EPOCHS+1,1):\n",
        "        if verbose:\n",
        "            # Print out an initial message stating the epoch number\n",
        "            # and learning rate\n",
        "            lrs = [group['lr'] for group in optimizer.param_groups]\n",
        "            lr = lrs[0] if len(lrs) > 0 else -1\n",
        "            # The end='' argument tells Python not to print a newline\n",
        "            # so we can extend this row with more information\n",
        "            # as we compute it.\n",
        "            print(f'| epoch {epoch:3d} | lr {lr:03.3f} | ', end='')\n",
        "\n",
        "        # Train for one epoch\n",
        "        loss, ppl = train_epoch(model, vocab_size, optimizer, train_data, parameters)\n",
        "\n",
        "        # Update the learning rate\n",
        "        scheduler.step(loss)\n",
        "        # Note that we can move this line after the validation step\n",
        "        # if we want to update the learning rate based on the val\n",
        "        # loss instead of the training loss.\n",
        "\n",
        "        if verbose:\n",
        "            # Update the printed row with the training loss and perplexity\n",
        "            # so that the user knows the training step completed and that\n",
        "            # we're now evaluating on the validation set.\n",
        "            print(f'train loss {loss:5.2f} | train ppl {ppl:8.2f} | ', end='')\n",
        "\n",
        "        # Evaluate on the validation set\n",
        "        val_loss, val_ppl = evaluate(model, vocab_size, validation_data, parameters)\n",
        "\n",
        "        if verbose:\n",
        "            # Update the printed row with the validation loss and perplexity\n",
        "            print(f'val loss {val_loss:5.2f} | val ppl {val_ppl:8.2f} |')\n",
        "            # We no longer have the end='' argument, so this completes the line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEe_MaYYACMO"
      },
      "outputs": [],
      "source": [
        "def generate_text(\n",
        "    tokenizer: tokenizer_toy.SimpleWordPieceTokenizer,\n",
        "    model: RecurrentModel,\n",
        "    input_text: str,\n",
        "    num_tokens_to_generate: int,\n",
        "    by_prob: bool = False,\n",
        ") -> str:\n",
        "    \"\"\"Input a text and use the model generate the next `n` words.\n",
        "\n",
        "    Args:\n",
        "        tokenizer (Tokenizer): The tokenizer used in training.\n",
        "        model (RNNModel): A trained recurrent neural network model.\n",
        "        text (str): A string of text to generate the text off of.\n",
        "        num_words (int, optional): The number of words to generate.\n",
        "            Defaults to 10.\n",
        "        by_prob (bool, optional): If `True`, words are randomly generated\n",
        "            based on probability. If `False`, words are generated based on\n",
        "            highest probability. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated text string.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set the model to evaluation mode (no Dropout and the like)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculation because we won't use it\n",
        "\n",
        "        # Process our input text\n",
        "        data = tokenize_dataset(input_text, tokenizer)\n",
        "        # Turn it into a batch for the model\n",
        "        x = data.reshape((-1, 1)).to(DEVICE)\n",
        "\n",
        "        state = None # Start with a blank state\n",
        "        output_tokens = [] # Keep track of the generated tokens\n",
        "        for _ in range(num_tokens_to_generate):\n",
        "            # Get the model's predicted next token\n",
        "            y_pred, state = model(x, state)\n",
        "            # We only care about the last token's predictions\n",
        "            last_word_logits = y_pred[-1][0]\n",
        "\n",
        "            # If we're generating by probability, we'll randomly choose a word\n",
        "            if by_prob:\n",
        "                # Normalize the logits so they form a probability distribution\n",
        "                p = (\n",
        "                    torch.nn.functional.softmax(last_word_logits, dim=0)\n",
        "                )\n",
        "                # Randomly choose the next word based on the probability\n",
        "                word_index = p.multinomial(num_samples=1).cpu().item()\n",
        "            else:\n",
        "                # Otherwise, we'll just choose the word with the highest probability\n",
        "                word_index = torch.argmax(last_word_logits)\n",
        "\n",
        "            # Add the generated word to our output\n",
        "            output_tokens.append(word_index)\n",
        "\n",
        "            # Add the generated word as the next input to the model\n",
        "            x = torch.tensor([word_index]).reshape(1, 1).to(DEVICE)\n",
        "\n",
        "    # Decode the generated tokens and return the result\n",
        "    return tokenizer.decode(output_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BOWWetqACMO"
      },
      "outputs": [],
      "source": [
        "def create_model(\n",
        "    vocab_size: int,\n",
        "    parameters: Parameters,\n",
        "    model_type: Callable[..., Union[RNN, LSTM, GRU]],\n",
        "    script: bool = True,\n",
        ") -> RecurrentModel:\n",
        "    \"\"\"Create a recurrent model.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): The number of vocabulary in the dataset.\n",
        "        parameters (Parameters): The model parameters.\n",
        "        model_type (Callable[..., Union[RNN, LSTM, GRU]]): The type of recurrent\n",
        "            model to create.\n",
        "\n",
        "    Returns:\n",
        "        RecurrentModel: The recurrent model.\n",
        "    \"\"\"\n",
        "    # Create the model\n",
        "    # We've split the model into two pieces:\n",
        "    # 1. The RecurrentModel class, which defines the overall model architecture\n",
        "    #    including the embedding and output layer\n",
        "    # 2. The RNN, LSTM, or GRU class, which defines the recurrent unit\n",
        "    #    that will be used inside the model.\n",
        "    # This allows us to easily swap out different recurrent units\n",
        "    # without having to rewrite the whole model's code\n",
        "    # across multiple notebooks or cells.\n",
        "\n",
        "    # The added \"torch.jit.script\" is a magic incantation that\n",
        "    # tells PyTorch to compile the model into a more efficient\n",
        "    # representation. This is not necessary, but it makes the\n",
        "    # model run faster and use less memory.\n",
        "\n",
        "    # The flatten_parameters() call is also not strictly necessary,\n",
        "    # but it makes the model run faster and use less GPU memory\n",
        "    # (and gets rid of a warning message).\n",
        "\n",
        "    instance = RecurrentModel(\n",
        "        vocab_size=vocab_size,\n",
        "        recurrent_module=model_type(\n",
        "            input_size=parameters.EMBEDDING_DIM,\n",
        "            hidden_size=parameters.HIDDEN_DIM,\n",
        "            num_layers=parameters.NUM_LAYERS,\n",
        "        ),\n",
        "    ).to(DEVICE)\n",
        "    instance.rnn.flatten_parameters()\n",
        "    if script:\n",
        "        instance = torch.jit.script(instance)\n",
        "    return instance\n",
        "\n",
        "def train_model(\n",
        "    model: RecurrentModel,\n",
        "    vocab_size: int,\n",
        "    unbatched_train_data: Tensor,\n",
        "    unbatched_validation_data: Tensor,\n",
        "    parameters: Parameters,\n",
        "    verbose: bool = True,\n",
        ") -> None:\n",
        "    if verbose:\n",
        "        print(f'The shape of the training data is {unbatched_train_data.shape}.')\n",
        "    batched_training_data = batchify(unbatched_train_data, parameters.BATCH_SIZE).to(DEVICE)\n",
        "    if verbose:\n",
        "        print(f'The shape of the batched training data is {batched_training_data.shape}.')\n",
        "        print(f'The shape of the validation data is {unbatched_validation_data.shape}.')\n",
        "    batched_validation_data = batchify(unbatched_validation_data, parameters.BATCH_SIZE).to(DEVICE)\n",
        "    if verbose:\n",
        "        print(f'The shape of the batched validation data is {batched_validation_data.shape}.')\n",
        "\n",
        "    train_run(model, vocab_size, batched_training_data, batched_validation_data, parameters, verbose)\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_and_train_model(\n",
        "    vocab_size: int,\n",
        "    parameters: Parameters,\n",
        "    model_type: Callable[..., Union[RNN, LSTM, GRU]],\n",
        "    unbatched_train_data: Tensor,\n",
        "    unbatched_validation_data: Tensor,\n",
        "    verbose: bool = True,\n",
        ") -> RecurrentModel:\n",
        "    \"\"\"Create and train a recurrent model.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): The number of vocabulary in the dataset.\n",
        "        parameters (Parameters): The model parameters.\n",
        "        model_type (Callable[..., Union[RNN, LSTM, GRU]]): The type of recurrent\n",
        "            model to create.\n",
        "        train_data (Tensor): Unbatched training data.\n",
        "\n",
        "    Returns:\n",
        "        RecurrentModel: The trained recurrent model.\n",
        "    \"\"\"\n",
        "    model = create_model(vocab_size, parameters, model_type)\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    train_model(model, unbatched_train_data, unbatched_validation_data, parameters, verbose)\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9bV_w1dACMO"
      },
      "source": [
        "## Run model-building experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkHctjX9ACMO"
      },
      "outputs": [],
      "source": [
        "parameters = Parameters(\n",
        "    BATCH_SIZE=32,\n",
        "    BPTT=50,\n",
        "    LR=0.01,\n",
        "    EPOCHS=30,\n",
        "    EMBEDDING_DIM=16,\n",
        "    HIDDEN_DIM=16,\n",
        "    NUM_LAYERS=3,\n",
        ")\n",
        "shakespeare_recurrent_module = GRU\n",
        "daily_dialog_recurrent_module = GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGC0xeUIACMP"
      },
      "outputs": [],
      "source": [
        "shakespeare_model = create_model(\n",
        "    vocab_size=tokenizer.get_vocab_size(),\n",
        "    parameters=parameters,\n",
        "    model_type=shakespeare_recurrent_module,\n",
        ")\n",
        "train_model(\n",
        "    model=shakespeare_model,\n",
        "    vocab_size=tokenizer.get_vocab_size(),\n",
        "    unbatched_train_data=shakespeare_train_tokens,\n",
        "    unbatched_validation_data=shakespeare_val_tokens,\n",
        "    parameters=parameters,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGzwtxBDACMP"
      },
      "outputs": [],
      "source": [
        "daily_dialog_model = create_model(\n",
        "    vocab_size=tokenizer.get_vocab_size(),\n",
        "    parameters=parameters,\n",
        "    model_type=daily_dialog_recurrent_module,\n",
        ")\n",
        "train_model(\n",
        "    model=daily_dialog_model,\n",
        "    vocab_size=tokenizer.get_vocab_size(),\n",
        "    unbatched_train_data=daily_dialog_train_tokens,\n",
        "    unbatched_validation_data=daily_dialog_val_tokens,\n",
        "    parameters=parameters,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItmqZ_wlACMP"
      },
      "source": [
        "## Save final models\n",
        "\n",
        "Just like with the Tokenizer, we can save the model so that we can load it later without having to retrain it, which will save us time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1soNNttACMP"
      },
      "outputs": [],
      "source": [
        "torch.save(shakespeare_model.state_dict(), \"shakespeare_model.pt\")\n",
        "torch.save(daily_dialog_model.state_dict(), \"daily_dialog_model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbEZ2rvEACMP"
      },
      "source": [
        "**Remember to download the model files and tokenizer together from Colab to preserve them!**\n",
        "\n",
        "(If running in a local Jupyter notebook instance, the files are already saved to your computer.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBt0w0hIACMP"
      },
      "source": [
        "### Reload the models\n",
        "\n",
        "To run this \"reload\"-ing code, we'll need the definition of `RecurrentModel` to be in scope. If you've restarted your runtime, you'll need to re-run the cell where you defined `RecurrentModel` before running the below cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOetAX8cACMP"
      },
      "outputs": [],
      "source": [
        "shakespeare_model = create_model(\n",
        "    vocab_size=tokenizer.get_vocab_size(),\n",
        "    parameters=parameters,\n",
        "    model_type=shakespeare_recurrent_module,\n",
        ")\n",
        "shakespeare_model.load_state_dict(torch.load(\"shakespeare_model.pt\"))\n",
        "daily_dialog_model = create_model(\n",
        "    vocab_size=tokenizer.get_vocab_size(),\n",
        "    parameters=parameters,\n",
        "    model_type=daily_dialog_recurrent_module,\n",
        ")\n",
        "daily_dialog_model.load_state_dict(torch.load(\"daily_dialog_model.pt\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yMpkBKfACMQ"
      },
      "source": [
        "## Test model by perplexity on datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHZq-sCXACMQ"
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"shakespeare_model\": shakespeare_model,\n",
        "    \"daily_dialog_model\": daily_dialog_model,\n",
        "}\n",
        "datasets = {\n",
        "    \"shakespeare_train\": shakespeare_train_tokens,\n",
        "    \"daily_dialog_train\": daily_dialog_train_tokens,\n",
        "    \"shakespeare_val\": shakespeare_val_tokens,\n",
        "    \"daily_dialog_val\": daily_dialog_val_tokens,\n",
        "}\n",
        "for model_name, model in models.items():\n",
        "    model.eval()\n",
        "    for dataset_name, dataset in datasets.items():\n",
        "        print(f\"{dataset_name} perplexity for {model_name}:\", end=\"\")\n",
        "        loss, ppl = evaluate(\n",
        "            model,\n",
        "            tokenizer.get_vocab_size(),\n",
        "            batchify(dataset, parameters.BATCH_SIZE).to(DEVICE),\n",
        "            parameters,\n",
        "        )\n",
        "        print(f\" {ppl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOwN1BdZACMQ"
      },
      "source": [
        "## Test model by generating text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Agu-pKeqACMQ"
      },
      "outputs": [],
      "source": [
        "print(generate_text(\n",
        "    tokenizer,\n",
        "    shakespeare_model,\n",
        "    \"courage under this? Who else would dare\",\n",
        "    200,\n",
        "    by_prob=True,\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qswi7qr8ACMQ"
      },
      "outputs": [],
      "source": [
        "print(generate_text(\n",
        "    tokenizer,\n",
        "    daily_dialog_model,\n",
        "    \"Hey, Thomas, could you run by the airport to pick up Ms. Davis?\",\n",
        "    200,\n",
        "    by_prob=True,\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl6qgdMgACMQ"
      },
      "source": [
        "## Classify text\n",
        "\n",
        "Language models can do more than just generating samples of similar text. They can also be used to classify text. We can do this by using the model to predict an \"emotion\" for each chunk of the `daily_dialog` dataset, then compare the predicted emotions to the labeled emotions from the dataset.\n",
        "\n",
        "### Prepare the dataset for classification\n",
        "\n",
        "First, we'll need to load the daily dialog dataset's emotion labels into a list of lists -- one list for every dialog, one entry for every utterance in the dialog. We'll also need to reload the text of the dataset, as we'll need to get the same list of lists pattern for the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9Biz1PfACMQ"
      },
      "source": [
        "#### Load the emotions datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5atkbq8SACMR"
      },
      "outputs": [],
      "source": [
        "def load_emotions_dataset(path: Union[str, Path]) -> List[int]:\n",
        "    \"\"\"Load the emotions dataset.\n",
        "\n",
        "    Args:\n",
        "        path (Union[str, Path]): The path to the emotions dataset.\n",
        "\n",
        "    Returns:\n",
        "        List[List[int]]: A list of lists of emotion IDs.\n",
        "    \"\"\"\n",
        "    with open(path, \"r\") as f:\n",
        "        return [int(emotion) for line in f.readlines() for emotion in line.split()]\n",
        "\n",
        "emotion_train = load_emotions_dataset(\"data/daily_dialog_train_emotion.txt\")\n",
        "emotion_val = load_emotions_dataset(\"data/daily_dialog_val_emotion.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iFNhUQIACMR"
      },
      "source": [
        "#### Match the emotions to the text\n",
        "\n",
        "We know each emotion in the emotions datasets should correspond to one appearance of `__eou__` in the text dataset. We'll use this to match the emotions to the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19JJTd_yACMR"
      },
      "outputs": [],
      "source": [
        "def scatter_emotions_to_text(text_tokens: Tensor, emotion_tokens: List[int]) -> Tensor:\n",
        "    # We'll return a tensor with the same shape as the text tokens but\n",
        "    # with nonzero values only where the `__eou__` tokens are. These\n",
        "    # nonzero values will be the corresponding emotion IDs.\n",
        "    result = torch.zeros_like(text_tokens, dtype=torch.uint8)\n",
        "    # Find the indices of the `__eou__` tokens\n",
        "    eou_indices = (text_tokens == tokenizer.get_vocab()[\"__eou__\"])\n",
        "    # Scatter the emotion IDs into the result tensor at the `__eou__` indices\n",
        "    result[eou_indices] = torch.tensor(emotion_tokens, dtype=torch.uint8)\n",
        "    return result\n",
        "\n",
        "emotion_train_tokens = scatter_emotions_to_text(daily_dialog_train_tokens, emotion_train)\n",
        "emotion_val_tokens = scatter_emotions_to_text(daily_dialog_val_tokens, emotion_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uRqfUVFACMR"
      },
      "source": [
        "### Replace the next-word prediction layer\n",
        "\n",
        "By replacing the next-word prediction layer with a classification layer, we can use the model to predict the emotion of each utterance in the dataset.\n",
        "\n",
        "We have 7 emotions in the dataset, so we'll need to replace the next-word prediction layer with a classification layer with 7 output units:\n",
        "\n",
        "0. `No Emotion` (or missing label)\n",
        "1. `Anger`\n",
        "2. `Disgust`\n",
        "3. `Fear`\n",
        "4. `Happiness`\n",
        "5. `Sadness`\n",
        "6. `Surprise`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZB5qIS57ACMR"
      },
      "outputs": [],
      "source": [
        "EMOTION_VOCAB_SIZE = 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsJ0kmIHACMR"
      },
      "outputs": [],
      "source": [
        "def create_fresh_emotion_classifier(\n",
        "    vocab_size: int,\n",
        "    parameters: Parameters,\n",
        "    model_type: Callable[..., Union[RNN, LSTM, GRU]],\n",
        ") -> RecurrentModel:\n",
        "    daily_dialog_model = create_model(\n",
        "        vocab_size=vocab_size,\n",
        "        parameters=parameters,\n",
        "        model_type=model_type,\n",
        "        script=False # We can't precompile the model if we're going to change it\n",
        "    )\n",
        "    daily_dialog_model.linear = Linear(parameters.HIDDEN_DIM, EMOTION_VOCAB_SIZE).to(DEVICE)\n",
        "    return torch.jit.script(daily_dialog_model)\n",
        "\n",
        "def create_text_pretrained_emotion_classifier(\n",
        "    vocab_size: int,\n",
        "    parameters: Parameters,\n",
        "    model_type: Callable[..., Union[RNN, LSTM, GRU]],\n",
        ") -> RecurrentModel:\n",
        "    daily_dialog_model = create_model(\n",
        "        vocab_size=vocab_size,\n",
        "        parameters=parameters,\n",
        "        model_type=model_type,\n",
        "        script=False # We can't precompile the model if we're going to change it\n",
        "    )\n",
        "    daily_dialog_model.load_state_dict(torch.load(\"daily_dialog_model.pt\"))\n",
        "    daily_dialog_model.linear = Linear(parameters.HIDDEN_DIM, EMOTION_VOCAB_SIZE).to(DEVICE)\n",
        "    return torch.jit.script(daily_dialog_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww6abEyUACMR"
      },
      "source": [
        "### Write our new training and evaluation functions\n",
        "\n",
        "These training functions are just like the ones we wrote for the language modeling task, but with a few key differences:\n",
        "\n",
        "* We now only count the loss when the input token is `__eou__`, as that is the only token that has a corresponding emotion label.\n",
        "* We now compute the accuracy of the model's predictions, as we can compare them to the emotion labels.\n",
        "  * This is instead of perplexity, as we are no longer predicting the next word in the sequence.\n",
        "\n",
        "Beyond these differences, the training functions are the same as before:\n",
        "\n",
        "- `train_epoch_classifier` trains the model for one epoch\n",
        "- `evaluate_classifier` evaluates the model on the validation set\n",
        "- `train_run_classifier` trains the model for all `EPOCHS` epochs and prints as it goes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcqeT_ZqACMS"
      },
      "outputs": [],
      "source": [
        "@torch.jit.script\n",
        "def get_classifier_batch(\n",
        "    source: Tensor,\n",
        "    emotions: Tensor,\n",
        "    i: int,\n",
        "    bptt: int\n",
        ") -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        emotions: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = emotions[i:i+seq_len].reshape(-1)\n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWJuibxHACMS"
      },
      "outputs": [],
      "source": [
        "def train_epoch_classifier(\n",
        "    model: RecurrentModel,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    train_data: Tensor,\n",
        "    target_data: Tensor,\n",
        "    parameters: Parameters,\n",
        "    eou_token_id: int,\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"Training function for our recurrent model.\n",
        "\n",
        "    Args:\n",
        "        model (RecurrentModel): The model to train.\n",
        "        optimizer (torch.optim.Optimizer): The optimizer to use.\n",
        "        train_data (Tensor): Batchified training data.\n",
        "        target_data (Tensor): Batchified target data.\n",
        "        parameters (Parameters): The model parameters.\n",
        "        eou_token_id (int): The ID of the `__eou__` token.\n",
        "    \"\"\"\n",
        "    # Turn on training mode which enables training-specific\n",
        "    # layer functionality (e.g. dropout)\n",
        "    model.train()\n",
        "\n",
        "    # Keep track of the loss as we go\n",
        "    total_loss: float = 0.0\n",
        "\n",
        "    # Keep track of the accuracy as we go\n",
        "    # TODO: Initialize variables to keep track of the accuracy here\n",
        "\n",
        "    # Determine how many steps we'll need to cover the whole dataset\n",
        "    num_batches: int = len(train_data) // parameters.BPTT\n",
        "\n",
        "    # Keep track of the hidden state between batches.\n",
        "    state = None\n",
        "\n",
        "    # For each of our batches\n",
        "    for i in range(0, train_data.size(0) - 1, parameters.BPTT):\n",
        "        # Throw away gradients from previous step\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get our input and target batches\n",
        "        data, targets = get_classifier_batch(\n",
        "            train_data, target_data, i, parameters.BPTT\n",
        "        )\n",
        "\n",
        "        # Evaluate the model to get logits and the new hidden state\n",
        "        output, state = model(data, state)\n",
        "\n",
        "        # Any tokens with an input token ID not of `__eou__` should be ignored\n",
        "        # in the loss. We'll create a mask to zero out the loss for these tokens.\n",
        "        mask = data.reshape(-1) == eou_token_id\n",
        "\n",
        "        # If the mask is all False, then we don't need to compute a loss\n",
        "        attempts = mask.sum().item()\n",
        "        if attempts == 0:\n",
        "            continue\n",
        "\n",
        "        # Compute the accuracy\n",
        "        correct = (output.argmax(dim=-1).reshape(-1)[mask] == targets[mask]).sum().item()\n",
        "        # TODO: Update the accuracy variables here\n",
        "\n",
        "        # Compute the loss\n",
        "        # Note that we're using the `reduction=\"none\"` argument here\n",
        "        # so that we can zero out the loss for tokens that are not\n",
        "        # `__eou__` tokens.\n",
        "        loss = torch.nn.functional.cross_entropy(\n",
        "            output.view(-1, EMOTION_VOCAB_SIZE), targets, reduction=\"none\"\n",
        "        )\n",
        "\n",
        "        # Now we do our own reduction.\n",
        "        loss = loss[mask].mean()\n",
        "\n",
        "        # Make sure the state does not carry gradients\n",
        "        # between batches so we avoid a runaway memory leak\n",
        "        model.detach_state_(state)\n",
        "\n",
        "        # Compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradients to avoid exploding gradients\n",
        "        # (Ask us TAs if you're curious why this is necessary)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "\n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Keep track of the loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Return the average loss across batches\n",
        "    cur_loss = total_loss / num_batches\n",
        "    # and the accuracy\n",
        "    # TODO: Compute the accuracy here\n",
        "    return cur_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIHOcsIuACMS"
      },
      "outputs": [],
      "source": [
        "# @torch.jit.script # If you're having trouble with this function, try commenting out this line.\n",
        "def evaluate_classifier(\n",
        "    model: RecurrentModel,\n",
        "    val_data: Tensor,\n",
        "    target_data: Tensor,\n",
        "    parameters: Parameters,\n",
        "    eou_token_id: int,\n",
        "    emotion_vocab_size: int,\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"Evaluate the perplexity of the model on the validation data.\n",
        "\n",
        "    Args:\n",
        "        model (RecurrentModel): The model to evaluate.\n",
        "        val_data (Tensor): Batchified validation data.\n",
        "\n",
        "    Returns:\n",
        "        float: The perplexity of the model on the validation data.\n",
        "    \"\"\"\n",
        "    # Keep track of the loss as we go\n",
        "    total_loss: float = 0.0\n",
        "\n",
        "    # Keep track of the accuracy as we go\n",
        "    # TODO: Initialize variables to keep track of the accuracy here\n",
        "\n",
        "    # Determine how many steps we'll need to cover the whole dataset\n",
        "    num_batches: int = len(val_data) // parameters.BPTT\n",
        "\n",
        "    # Keep track of the hidden state between batches.\n",
        "    state = None\n",
        "    with torch.no_grad():  # No need to track gradients here, since we're not training\n",
        "        for i in range(0, val_data.size(0) - 1, parameters.BPTT):\n",
        "            # Get our input and target batches\n",
        "            data, targets = get_classifier_batch(val_data, target_data, i, parameters.BPTT)\n",
        "\n",
        "            # Evaluate the model to get logits and the new hidden state\n",
        "            output, state = model.forward(data, state)\n",
        "\n",
        "            # Compute our new mask\n",
        "            mask = data.reshape(-1) == eou_token_id\n",
        "\n",
        "            # If the mask is all False, then we don't need to compute a loss\n",
        "            attempts = mask.sum().item()\n",
        "            if attempts == 0:\n",
        "                continue\n",
        "\n",
        "            # Compute the accuracy\n",
        "            correct = (output.argmax(dim=-1).reshape(-1)[mask] == targets[mask]).sum().item()\n",
        "            # TODO: Update the accuracy variables here\n",
        "\n",
        "            # Compute the loss\n",
        "            # Note that we're using the `reduction=\"none\"` argument here\n",
        "            # so that we can zero out the loss for tokens that are not\n",
        "            # `__eou__` tokens.\n",
        "            loss = torch.nn.functional.cross_entropy(output.view(-1, emotion_vocab_size), targets, reduction=\"none\")\n",
        "            loss = loss[mask].mean() # Now we do our own reduction.\n",
        "\n",
        "            # Keep track of the total loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    # Return the average loss across batches\n",
        "    cur_loss = total_loss / num_batches\n",
        "    # and the accuracy\n",
        "    # TODO: Compute the final accuracy here\n",
        "    accuracy = total_correct / total_attempts\n",
        "    return cur_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aph5rIZrACMS"
      },
      "outputs": [],
      "source": [
        "def train_run_classifier(\n",
        "    model: RecurrentModel,\n",
        "    train_data: Tensor,\n",
        "    train_target_data: Tensor,\n",
        "    validation_data: Tensor,\n",
        "    validation_target_data: Tensor,\n",
        "    eou_token_id: int,\n",
        "    parameters: Parameters,\n",
        "    verbose: bool = True,\n",
        ") -> None:\n",
        "    \"\"\"Train a model for NUM_EPOCHS epochs.\"\"\"\n",
        "    # Here we define an optimizer and scheduler to use for training.\n",
        "    # An optimizer adjusts the parameters of the model based on the loss\n",
        "    # and the gradients of the parameters with respect to the loss.\n",
        "\n",
        "    # Adam is a popular optimizer that tends to work well in practice.\n",
        "    # optimizer = torch.optim.Adam(model.parameters(), lr=parameters.LR) # Set LR to about 0.005 to start\n",
        "\n",
        "    # AdamW corrects a minor flaw in the original Adam implementation\n",
        "    # allowing it to converge slightly better.\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), lr=parameters.LR\n",
        "    )  # Set LR to about 0.01\n",
        "\n",
        "    # SGD is the simple stochastic gradient descent algorithm,\n",
        "    # implemented as LR * gradient.\n",
        "    # Notably, the RNNs shown here need a strong learning rate to\n",
        "    # even begin to move the loss, so we start with a high LR.\n",
        "    # optimizer = torch.optim.SGD(model.parameters(), lr=parameters.LR) # Set LR to about 10.0 to start\n",
        "\n",
        "    # The scheduler adjusts the learning rate over time.\n",
        "    # Here we use a scheduler that decreases the learning rate\n",
        "    # by a factor of 0.5 if the training loss doesn't decrease\n",
        "    # for two epochs in a row.\n",
        "    # This makes sure that the model can converge down into a\n",
        "    # minimum once it finds one, instead of bouncing around.\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, factor=0.5, patience=1, threshold=1e-3\n",
        "    )\n",
        "\n",
        "    # Train for NUM_EPOCHS epochs\n",
        "    for epoch in range(1, parameters.EPOCHS + 1, 1):\n",
        "        if verbose:\n",
        "            # Print out an initial message stating the epoch number\n",
        "            # and learning rate\n",
        "            lrs = [group[\"lr\"] for group in optimizer.param_groups]\n",
        "            lr = lrs[0] if len(lrs) > 0 else -1\n",
        "            # The end='' argument tells Python not to print a newline\n",
        "            # so we can extend this row with more information\n",
        "            # as we compute it.\n",
        "            print(f\"| epoch {epoch:3d} | lr {lr:05.5f} | \", end=\"\")\n",
        "\n",
        "        # Train for one epoch\n",
        "        loss, acc = train_epoch_classifier(\n",
        "            model,\n",
        "            optimizer,\n",
        "            train_data,\n",
        "            train_target_data,\n",
        "            eou_token_id=eou_token_id,\n",
        "            parameters=parameters,\n",
        "        )\n",
        "\n",
        "        # Update the learning rate\n",
        "        scheduler.step(loss)\n",
        "        # Note that we can move this line after the validation step\n",
        "        # if we want to update the learning rate based on the val\n",
        "        # loss instead of the training loss.\n",
        "\n",
        "        if verbose:\n",
        "            # Update the printed row with the training loss and perplexity\n",
        "            # so that the user knows the training step completed and that\n",
        "            # we're now evaluating on the validation set.\n",
        "            print(f\"train loss {loss:5.4f} | train acc {acc*100:5.2f} | \", end=\"\")\n",
        "\n",
        "        # Evaluate on the validation set\n",
        "        model.eval()\n",
        "        val_loss, val_acc = evaluate_classifier(\n",
        "            model,\n",
        "            validation_data,\n",
        "            validation_target_data,\n",
        "            eou_token_id=eou_token_id,\n",
        "            parameters=parameters,\n",
        "            emotion_vocab_size=EMOTION_VOCAB_SIZE,\n",
        "        )\n",
        "\n",
        "        if verbose:\n",
        "            # Update the printed row with the validation loss and perplexity\n",
        "            print(f\"val loss {val_loss:5.4f} | val acc {val_acc*100:5.2f} |\")\n",
        "            # We no longer have the end='' argument, so this completes the line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H8cPC6bACMT"
      },
      "outputs": [],
      "source": [
        "def train_model_classifier(\n",
        "    model: RecurrentModel,\n",
        "    unbatched_train_data: Tensor,\n",
        "    unbatched_train_target_data: Tensor,\n",
        "    unbatched_validation_data: Tensor,\n",
        "    unbatched_validation_target_data: Tensor,\n",
        "    eou_token_id: int,\n",
        "    parameters: Parameters,\n",
        "    verbose: bool = True,\n",
        ") -> None:\n",
        "    if verbose:\n",
        "        print(\n",
        "            f\"The shape of the training data is {unbatched_train_data.shape} and {unbatched_train_target_data.shape}.\"\n",
        "        )\n",
        "    batched_training_data = batchify(\n",
        "        unbatched_train_data, parameters.BATCH_SIZE\n",
        "    ).to(DEVICE)\n",
        "    batched_training_target_data = batchify(\n",
        "        unbatched_train_target_data, parameters.BATCH_SIZE\n",
        "    ).to(DEVICE)\n",
        "    if verbose:\n",
        "        print(\n",
        "            f\"The shape of the batched training data is {batched_training_data.shape} and {batched_training_target_data.shape}.\"\n",
        "        )\n",
        "        print(\n",
        "            f\"The shape of the validation data is {unbatched_validation_data.shape} and {unbatched_validation_target_data.shape}.\"\n",
        "        )\n",
        "    batched_validation_data = batchify(\n",
        "        unbatched_validation_data, parameters.BATCH_SIZE\n",
        "    ).to(DEVICE)\n",
        "    batched_validation_target_data = batchify(\n",
        "        unbatched_validation_target_data, parameters.BATCH_SIZE\n",
        "    ).to(DEVICE)\n",
        "    if verbose:\n",
        "        print(\n",
        "            f\"The shape of the batched validation data is {batched_validation_data.shape} and {batched_validation_target_data.shape}.\"\n",
        "        )\n",
        "\n",
        "    train_run_classifier(\n",
        "        model,\n",
        "        batched_training_data,\n",
        "        batched_training_target_data,\n",
        "        batched_validation_data,\n",
        "        batched_validation_target_data,\n",
        "        eou_token_id=eou_token_id,\n",
        "        parameters=parameters,\n",
        "        verbose=verbose,\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca951lcoACMT"
      },
      "source": [
        "### Run model building experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaSqQ1aFACMT"
      },
      "outputs": [],
      "source": [
        "# Redefine the parameters object so you can play with them, if desired.\n",
        "# When training the classifier, as we're only including some tokens in the loss,\n",
        "# the calculations are less intensive and we can support a larger batch size.\n",
        "# You may also want to play with Learning Rate.\n",
        "classifier_parameters = Parameters(\n",
        "    BATCH_SIZE=4*parameters.BATCH_SIZE,\n",
        "    BPTT=parameters.BPTT,\n",
        "    LR=parameters.LR,\n",
        "    EPOCHS=2*parameters.EPOCHS,\n",
        "    EMBEDDING_DIM=parameters.EMBEDDING_DIM,\n",
        "    HIDDEN_DIM=parameters.HIDDEN_DIM,\n",
        "    NUM_LAYERS=parameters.NUM_LAYERS,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "me1uLlTsACMT"
      },
      "outputs": [],
      "source": [
        "fresh_classifier_model = create_fresh_emotion_classifier(\n",
        "    vocab_size=tokenizer.get_vocab_size(),\n",
        "    parameters=classifier_parameters,\n",
        "    model_type=daily_dialog_recurrent_module,\n",
        ")\n",
        "train_model_classifier(\n",
        "    fresh_classifier_model,\n",
        "    daily_dialog_train_tokens,\n",
        "    emotion_train_tokens,\n",
        "    daily_dialog_val_tokens,\n",
        "    emotion_val_tokens,\n",
        "    eou_token_id=tokenizer.token_to_id(\"__eou__\"),\n",
        "    parameters=classifier_parameters,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT0iVXvTACMT"
      },
      "outputs": [],
      "source": [
        "text_pretrained_classifier_model = create_text_pretrained_emotion_classifier(\n",
        "    vocab_size=tokenizer.get_vocab_size(),\n",
        "    parameters=classifier_parameters,\n",
        "    model_type=daily_dialog_recurrent_module,\n",
        ")\n",
        "train_model_classifier(\n",
        "    text_pretrained_classifier_model,\n",
        "    daily_dialog_train_tokens,\n",
        "    emotion_train_tokens,\n",
        "    daily_dialog_val_tokens,\n",
        "    emotion_val_tokens,\n",
        "    eou_token_id=tokenizer.token_to_id(\"__eou__\"),\n",
        "    parameters=classifier_parameters,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYwqZ01UACMT"
      },
      "source": [
        "## Evaluate model performance\n",
        "\n",
        "A key step in model building is evaluating the model is actually learning the problem we want it to learn, rather than discovering a bias in the data. We can do this by evaluating properties of the model's predictions, such as per class accuracy and confusion matrices.\n",
        "\n",
        "First, we'll need to write a function to get the model's predictions for a tokenized dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmSGquBzACMU"
      },
      "outputs": [],
      "source": [
        "def predict_emotions(model: RecurrentModel, input: Tensor, eou_token_id: int, bptt: int) -> List[int]:\n",
        "    \"\"\"Predict the emotions of a text in the `daily_dialog` format.\n",
        "\n",
        "    Args:\n",
        "        model (RecurrentModel): The emotion classifier.\n",
        "        input (Tensor): The text to predict the emotions of.\n",
        "        batch_size (int): The batch size to use during processing.\n",
        "\n",
        "    Returns:\n",
        "        List[int]: The predicted emotion IDs.\n",
        "    \"\"\"\n",
        "    # We need to use a batch size of 1 here because we need to ensure\n",
        "    # no tokens are ignored when computing our predictions.\n",
        "    batch_size = 1\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation while not training\n",
        "\n",
        "        # Process our input text\n",
        "        batched_input = batchify(input, batch_size).to(DEVICE)\n",
        "\n",
        "        # Keep a list of the predicted emotions\n",
        "        predicted_emotions = []\n",
        "\n",
        "        # Keep track of the hidden state between batches.\n",
        "        state = None\n",
        "\n",
        "        # For each of our batches\n",
        "        for i in range(0, batched_input.size(0) - 1, bptt):\n",
        "            # Get our input batch\n",
        "            data = batched_input[i:i+bptt]\n",
        "\n",
        "            # Evaluate the model to get logits and the new hidden state\n",
        "            output, state = model.forward(data, state)\n",
        "\n",
        "            # Compute our mask to ignore tokens that are not `__eou__` tokens\n",
        "            mask = data.reshape(-1) == eou_token_id\n",
        "\n",
        "            # Get the predicted emotion for each token\n",
        "            predicted_emotions.extend(output.argmax(dim=-1).reshape(-1)[mask].cpu().tolist())\n",
        "\n",
        "    # Decode the generated tokens and return the result\n",
        "    return predicted_emotions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktGAhKQBACMU"
      },
      "outputs": [],
      "source": [
        "fresh_classifier_model.eval()\n",
        "fresh_classifier_model_predictions = predict_emotions(\n",
        "    fresh_classifier_model,\n",
        "    daily_dialog_val_tokens,\n",
        "    eou_token_id=tokenizer.token_to_id(\"__eou__\"),\n",
        "    bptt=parameters.BPTT,\n",
        ")\n",
        "print(\"fresh_classifier_model\", len(fresh_classifier_model_predictions), \"predictions\")\n",
        "text_pretrained_classifier_model.eval()\n",
        "text_pretrained_classifier_model_predictions = predict_emotions(\n",
        "    text_pretrained_classifier_model,\n",
        "    daily_dialog_val_tokens,\n",
        "    eou_token_id=tokenizer.token_to_id(\"__eou__\"),\n",
        "    bptt=parameters.BPTT,\n",
        ")\n",
        "print(\"text_pretrained_classifier_model\", len(text_pretrained_classifier_model_predictions), \"predictions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh1z6OhpACMU"
      },
      "source": [
        "### Per-class accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqyQ5dFhACMU"
      },
      "outputs": [],
      "source": [
        "emotion_counts = Counter(emotion_val)\n",
        "\n",
        "def per_class_accuracy(\n",
        "    predicted_emotions: List[int], actual_emotions: List[int]\n",
        ") -> Dict[int, float]:\n",
        "    \"\"\"Compute the per-class accuracy of the predicted emotions.\n",
        "\n",
        "    Args:\n",
        "        predicted_emotions (List[int]): The predicted emotion IDs.\n",
        "        actual_emotions (List[int]): The actual emotion IDs.\n",
        "\n",
        "    Returns:\n",
        "        Dict[int, float]: A dictionary mapping emotion IDs to accuracies.\n",
        "    \"\"\"\n",
        "    # Create a dictionary mapping emotion IDs to the number of times\n",
        "    # we've seen that emotion ID in the actual emotions\n",
        "    # emotion_counts = Counter(actual_emotions)\n",
        "    # Moved this outside the function so we don't have to recompute it\n",
        "\n",
        "    # Create a dictionary mapping emotion IDs to the number of times\n",
        "    # we've seen that emotion ID in the actual emotions and predicted\n",
        "    # that emotion ID correctly\n",
        "    correct_counts = defaultdict(int)\n",
        "    for predicted, actual in zip(predicted_emotions, actual_emotions):\n",
        "        if predicted == actual:\n",
        "            correct_counts[actual] += 1\n",
        "\n",
        "    # Compute the accuracy for each emotion ID\n",
        "    accuracies = {}\n",
        "    for emotion, count in emotion_counts.items():\n",
        "        accuracies[emotion] = correct_counts[emotion]*100 / count\n",
        "\n",
        "    return accuracies\n",
        "\n",
        "total_values = len(emotion_val)\n",
        "print(\"Dataset balance:\", {k: v/total_values for k, v in emotion_counts.items()})\n",
        "\n",
        "fresh_classifier_model_accuracies = per_class_accuracy(\n",
        "    fresh_classifier_model_predictions, emotion_val\n",
        ")\n",
        "print(\"fresh_classifier_model per-class accuracies:\", fresh_classifier_model_accuracies)\n",
        "\n",
        "text_pretrained_classifier_model_accuracies = per_class_accuracy(\n",
        "    text_pretrained_classifier_model_predictions, emotion_val\n",
        ")\n",
        "print(\"text_pretrained_classifier_model per-class accuracies:\", text_pretrained_classifier_model_accuracies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba-kiOBAACMU"
      },
      "source": [
        "### Confusion matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge4bQwC8ACMU"
      },
      "outputs": [],
      "source": [
        "def create_confusion_matrix(\n",
        "    predict_emotions: List[int], actual_emotions: List[int]\n",
        ") -> List[List[int]]:\n",
        "    \"\"\"Create a confusion matrix of the predicted emotions.\n",
        "\n",
        "    Args:\n",
        "        predict_emotions (List[int]): The predicted emotion IDs.\n",
        "        actual_emotions (List[int]): The actual emotion IDs.\n",
        "\n",
        "    Returns:\n",
        "        List[List[int]]: A confusion matrix.\n",
        "    \"\"\"\n",
        "    matrix = [[0 for _ in range(EMOTION_VOCAB_SIZE)] for _ in range(EMOTION_VOCAB_SIZE)]\n",
        "    for predicted, actual in zip(predict_emotions, actual_emotions):\n",
        "        matrix[predicted][actual] += 1\n",
        "    return matrix\n",
        "\n",
        "def print_confusion_matrix(\n",
        "    matrix: List[List[int]],\n",
        "    emotion_vocab: List[str]\n",
        ") -> None:\n",
        "    \"\"\"Print a confusion matrix.\n",
        "\n",
        "    Args:\n",
        "        matrix (List[List[int]]): The confusion matrix.\n",
        "        emotion_vocab (Dict[str, int]): The emotion vocabulary.\n",
        "    \"\"\"\n",
        "    label_len = max(4,max(len(label) for label in emotion_vocab))\n",
        "    print(\"Predicted\\Actual\")\n",
        "    print(\" \"*label_len, end=\"\")\n",
        "    for emotion in emotion_vocab:\n",
        "        print(f\"{emotion:>{label_len}s}\", end=\"\")\n",
        "    print(f\"  {'Total':>{label_len}s}\")\n",
        "    for i, row in enumerate(matrix):\n",
        "        print(f\"{emotion_vocab[i]:{label_len}s}\", end=\"\")\n",
        "        for col in row:\n",
        "            print(f\"{col:{label_len}d}\", end=\"\")\n",
        "        print(\n",
        "            f\"  {sum(row):{label_len}d}\"\n",
        "        )\n",
        "\n",
        "    # Total row\n",
        "    print(f\"{'Total':{label_len}s}\", end=\"\")\n",
        "    for col in range(len(matrix[0])):\n",
        "        print(f\"{sum(row[col] for row in matrix):{label_len}d}\", end=\"\")\n",
        "    print()\n",
        "\n",
        "emotion_vocab = [\"None\", \"Anger\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\"]\n",
        "\n",
        "\n",
        "print(\"fresh_classifier_model confusion matrix:\")\n",
        "print_confusion_matrix(\n",
        "    create_confusion_matrix(fresh_classifier_model_predictions, emotion_val),\n",
        "    emotion_vocab,\n",
        ")\n",
        "print(\"\\ntext_pretrained_classifier_model confusion matrix:\")\n",
        "print_confusion_matrix(\n",
        "    create_confusion_matrix(text_pretrained_classifier_model_predictions, emotion_val),\n",
        "    emotion_vocab,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRlskMZbACMV"
      },
      "source": [
        "## Compute Emotions on unseen test data\n",
        "\n",
        "Now that we've trained a model to classify the emotions of utterances in the `daily_dialog` training and validation datasets, we can use it to classify the emotions of utterances in an unseen test dataset.\n",
        "\n",
        "Finish the below code to load the test dataset, tokenize it, and run it through the model to predict the emotions of each utterance. Then save the predictions to \"daily_dialog_test_emotion_predictions.csv\" in a file of newline-separated integers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhCyQAaoACMV"
      },
      "source": [
        "#### Choose a classifier to deploy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SYBah_EACMV"
      },
      "outputs": [],
      "source": [
        "better_classifier = (\n",
        "    # TODO: Choose the classifier you've decided is better\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEX6kTQPACMV"
      },
      "source": [
        "#### Load the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ldv_S3ZgACMV"
      },
      "outputs": [],
      "source": [
        "daily_dialog_test_text = load_dataset(\"data/daily_dialog_test_text.txt\")\n",
        "daily_dialog_test_tokens = tokenize_dataset(\n",
        "    daily_dialog_test_text, tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_DcUGOlACMV"
      },
      "source": [
        "#### Run the classifier on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NRYN-eTACMV"
      },
      "outputs": [],
      "source": [
        "predictions = predict_emotions(\n",
        "    better_classifier,\n",
        "    daily_dialog_test_tokens,\n",
        "    eou_token_id=tokenizer.token_to_id(\"__eou__\"),\n",
        "    bptt=parameters.BPTT,\n",
        ")\n",
        "print(\"fresh_classifier_model\", len(predictions), \"predictions\")\n",
        "assert len(predictions) == 7740, f\"The number of predictions should be 7740 but was {len(predictions)}.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHmllI1zACMW"
      },
      "source": [
        "#### Save the predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7EPqrTTACMW"
      },
      "outputs": [],
      "source": [
        "with open(\"daily_dialog_test_emotion_predictions.csv\", \"w\", encoding=\"utf-8\") as outfile:\n",
        "    outfile.write(\"emotion\\n\")\n",
        "    for prediction in predictions:\n",
        "        outfile.write(f\"{prediction}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doEsNVPlACMW"
      },
      "source": [
        "**Remember to download the predictions file from Colab to preserve it!**\n",
        "\n",
        "Also remember to submit your code notebook to Gradescope!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}