{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TYXmeMH_aXV"
      },
      "source": [
        "# ECE 157B/272B Homework 1 Code\n",
        "Created by: Min Jian Yang and Matthew Dupree\\\n",
        "Parts of the code are from:\n",
        "- The PyTorch tutorial: https://pytorch.org/tutorials/beginner/transformer_tutorial.html#load-and-batch-data\n",
        "- The KDnuggets tutorial: https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDmkvaC2_aXX"
      },
      "source": [
        "## Select a GPU Runtime\n",
        "* If you would like to complete this assignment on your own computer, make sure that your computer has a GPU compatible with PyTorch hardware acceleration. Without a GPU, training the models will take a very long time.\n",
        "* If you are using Google Colab, you will want to limit the amount of time you spend using a GPU runtime, to prevent running up against usage limits. If you are planning to train models this session, (i.e. beyond just vocabulary exploration,) you will need to select a GPU runtime by going to `Runtime` > `Change runtime type` > `Hardware accelerator` > `GPU`.\n",
        "    * For each model you intend to train and evaluate, you will need 10-15 minutes of GPU runtime, so plan in advance.\n",
        "    * If your computer goes to sleep or loses its internet connection for more than a minute or two, your Google Colab runtime will be recycled, erasing your model and training progress.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMZlONxL_aXX"
      },
      "source": [
        "## Install Libraries\n",
        "The below lab will check the versions of the PyTorch and Pandas libraries you have installed. We're expecting to see these versions of these packages, but later versions may also work fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SdMSq3I_aXX"
      },
      "outputs": [],
      "source": [
        "%pip install torch==2.1.2 scikit-learn==1.4.0 tokenizers==0.15.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOMaQz1E_aXY"
      },
      "source": [
        "You may be asked to restart the kernel after running the above cell. If so, please do so and then continue running the cells below. (This means specifically to \"Restart\" the _kernel_, not to get a new runtime, as that will remove the package versions installed.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl82XHO__aXY"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dspmj8oN_aXY"
      },
      "outputs": [],
      "source": [
        "from typing import List, Callable, Tuple, NamedTuple, Union, Optional, Type\n",
        "from pathlib import Path\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, AddedToken, normalizers\n",
        "\n",
        "from torch.nn import Module\n",
        "from torch.nn import RNN, LSTM, GRU, Embedding, Linear\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYx5PEvn_aXY"
      },
      "source": [
        "## Use GPU if available\n",
        "The below line checks if a CUDA environment (typically a GPU accelerator) is available. If so, we'll use it, otherwise we'll use the CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEHoUWeL_aXZ",
        "outputId": "4183c6bc-039c-48ad-c6f6-f6807c246335"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: {DEVICE}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2tyBfW4_aXZ"
      },
      "source": [
        "## Parameters to tune for your experiments\n",
        "The block below defines a \"Parameters\" class that you can use to configure settings for training runs of your various models. We will guide you through creating training functions that will make use of these parameters to allow you to create mostly-reusable code.\n",
        "\n",
        "We'll also define some global parameters that will be used across all of your models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AB82-Ik_aXZ"
      },
      "outputs": [],
      "source": [
        "# Test set size as a fraction of the total dataset size\n",
        "TEST_SIZE: float = 0.05\n",
        "\n",
        "# Random seed for reproducibility\n",
        "RANDOM_STATE: int = 42\n",
        "\n",
        "# Vocabulary size - number of tokens to use to break up the text (and learn)\n",
        "VOCAB_SIZE_CAP: int = 19999\n",
        "\n",
        "# Minimum token frequency - tokens that appear less than this number of times will be ignored\n",
        "# (So single-use complex words will be ignored)\n",
        "MIN_TOKEN_FREQUENCY: int = 2\n",
        "# If no tokens appear at least this many times, the vocabulary size will be reduced\n",
        "# below VOCAB_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YavOPTTQ_aXZ"
      },
      "outputs": [],
      "source": [
        "class Parameters(NamedTuple):\n",
        "    ### Dataset and training parameters\n",
        "    # How many different chunks of text to train on at the same time\n",
        "    BATCH_SIZE: int\n",
        "    # Token target -- how many tokens to backpropagate through at most\n",
        "    BPTT: int\n",
        "    # Learning rate\n",
        "    LR: float\n",
        "    # Number of epochs to train for\n",
        "    EPOCHS: int\n",
        "\n",
        "    ### Model parameters\n",
        "    # Number of dimensions in the embedding of each token.\n",
        "    #   More dimensions --> more meaning, but more computation\n",
        "    EMBEDDING_DIM: int\n",
        "    # Number of dimensions in the hidden state in the recurrent model\n",
        "    HIDDEN_DIM: int\n",
        "    # Number of hidden layers in the recurrent model\n",
        "    NUM_LAYERS: int"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRYEmcp5_aXZ"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyDdAYM0_aXZ"
      },
      "outputs": [],
      "source": [
        "def load_data(path: Path) -> List[List[str]]:\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    def preprocess_line(line: str) -> str:\n",
        "        # TODO: Do any preprocessing here\n",
        "        #   e.g. removing punctuation, lowercasing, etc.\n",
        "        # If a line is empty after preprocessing,\n",
        "        # it will be removed from the dataset.\n",
        "        return line\n",
        "\n",
        "    # Each play is delineated by a line beginning '''\n",
        "    # We can use this to split the data into plays,\n",
        "    # then choose some of them to be training data\n",
        "    # and some to be validation data\n",
        "    plays = []\n",
        "    current_play = []\n",
        "    for line in lines:\n",
        "        if line.startswith('\\'\\'\\'') and current_play:\n",
        "            plays.append(current_play)\n",
        "            current_play = []\n",
        "\n",
        "        processed_line = preprocess_line(line)\n",
        "        if processed_line:\n",
        "            current_play.append(processed_line)\n",
        "\n",
        "    if current_play:\n",
        "        # The last play won't be added in the loop\n",
        "        plays.append(current_play)\n",
        "\n",
        "    return plays\n",
        "\n",
        "\n",
        "data_path = Path('shakespeare.txt')\n",
        "data_list = load_data(data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikyBGjU9_aXZ"
      },
      "source": [
        "## Split Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TmSo6CG_aXa"
      },
      "outputs": [],
      "source": [
        "def split_data(all_plays: List[List[str]]) -> Tuple[List[str], List[str]]:\n",
        "    # We use the scikit-learn train_test_split function\n",
        "    # just like we did in previous assignments.\n",
        "    # As we only provide one list, it splits that list\n",
        "    # into two pieces, one as TEST_SIZE fraction of the total data\n",
        "    # and the other as the rest.\n",
        "    training_plays, validation_plays = train_test_split(\n",
        "        all_plays,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=RANDOM_STATE,\n",
        "    )\n",
        "\n",
        "    # We'll append the plays in each set together into one big list of lines\n",
        "    training_list = [line for play in training_plays for line in play]\n",
        "    validation_list = [line for play in validation_plays for line in play]\n",
        "\n",
        "    return training_list, validation_list\n",
        "\n",
        "training_list, validation_list = split_data(data_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO9B_1OA_aXa"
      },
      "source": [
        "## Prepare the dataset for training\n",
        "\n",
        "### Build vocabulary\n",
        "\n",
        "* Break the given text into \"tokens\" using the tokenizer provided by the `tokenizers` library.\n",
        "* Build a vocabulary of tokens from the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOJTznI4_aXa",
        "outputId": "e9c3829b-8246-42a4-bad9-790bf7a9e1f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Newline added: 1\n"
          ]
        }
      ],
      "source": [
        "# We'll create a \"WordPiece\" tokenizer -- this is the kind used by BERT.\n",
        "tokenizer = Tokenizer(models.WordPiece(unk_token=\"<unk>\"))\n",
        "\n",
        "# This sequence of normalizers is the same as the one used by BERT.\n",
        "#  NFD: Normalization Form D (canonical decomposition)\n",
        "#       This is fancy Unicode speak for transforming accented characters\n",
        "#       into their base form and the accent separately.\n",
        "#       e.g. é -> e + ´\n",
        "#  Lowercase: Lowercase the text\n",
        "#  StripAccents: Remove the accents from the text (e.g. ´ from é)\n",
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
        ")\n",
        "\n",
        "# We'll use a whitespace pre-tokenizer.\n",
        "# This will split the text on whitespace and punctuation first,\n",
        "# then tokenize the words separately using the WordPiece model.\n",
        "# That way, we can keep punctuation as separate tokens,\n",
        "# which might be useful for the model.\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "# pre_tokenizers.WhitespaceSplit() would also work here,\n",
        "# but it would keep punctuation attached to words,\n",
        "# which might not be what we want.\n",
        "\n",
        "# Finally, we'll use the WordPiece decoder\n",
        "# to convert the tokens back into text.\n",
        "tokenizer.decoder = decoders.WordPiece()\n",
        "\n",
        "# We'll train the tokenizer on the training data.\n",
        "# You can play with the VOCAB_SIZE and MIN_TOKEN_FREQUENCY\n",
        "# parameters to see how they affect the tokenizer.\n",
        "trainer = trainers.WordPieceTrainer(\n",
        "    vocab_size=VOCAB_SIZE_CAP,\n",
        "    min_frequency=MIN_TOKEN_FREQUENCY,\n",
        "    special_tokens=[\"<unk>\"],\n",
        "    show_progress=True,\n",
        ")\n",
        "\n",
        "# This training process isn't so bad for Shakespeare,\n",
        "# but it can take a while for larger datasets scraped from the web.\n",
        "tokenizer.train_from_iterator(training_list, trainer=trainer)\n",
        "\n",
        "# Finally, we add a newline token to the tokenizer\n",
        "# so that the model can learn to predict line breaks\n",
        "# (and thus line lengths) instead of smushing it all\n",
        "# together into one giant line with character names\n",
        "# and stage directions and everything.\n",
        "print(\"Newline added:\",tokenizer.add_tokens([AddedToken(\"\\n\", normalized=False)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RzZYZ0P_aXb",
        "outputId": "78aec0da-594d-464d-b593-6bb74dd56e39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The vocabulary size is 19864.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'##apped': 13196,\n",
              " '##orters': 16429,\n",
              " 'laments': 10870,\n",
              " 'distaff': 12314,\n",
              " 'mummy': 18850,\n",
              " 'bags': 7885,\n",
              " '##by': 683,\n",
              " 'tough': 8748,\n",
              " 'bestowed': 7884,\n",
              " 'sunburn': 14813,\n",
              " 'babble': 15062,\n",
              " 'brute': 16847,\n",
              " 'shaft': 7410,\n",
              " 'foundation': 11766,\n",
              " '##flies': 12044,\n",
              " 'empir': 17856,\n",
              " 'lectures': 15743,\n",
              " 'strains': 10069,\n",
              " 'ravish': 5573,\n",
              " '##uster': 14098,\n",
              " 'cons': 1298,\n",
              " 'angels': 4634,\n",
              " 'ja': 1235,\n",
              " '##enders': 10573,\n",
              " 'prec': 8494,\n",
              " 'unreverend': 15851,\n",
              " '##eth': 930,\n",
              " 'bruit': 14333,\n",
              " 'deluge': 17785,\n",
              " 'jerkin': 7961,\n",
              " '##ool': 2764,\n",
              " 'falstaff': 479,\n",
              " 'sund': 5629,\n",
              " 'det': 3308,\n",
              " 'jar': 9747,\n",
              " 'lionel': 11858,\n",
              " 'frugal': 16653,\n",
              " 'hatred': 7059,\n",
              " 'windy': 11735,\n",
              " 'alenc': 2934,\n",
              " 'thither': 2828,\n",
              " 'salutes': 12827,\n",
              " 'unve': 12672,\n",
              " 'varlet': 5579,\n",
              " 'beseems': 15579,\n",
              " 'matter': 1139,\n",
              " '##sc': 980,\n",
              " 'mows': 13932,\n",
              " 'hive': 11296,\n",
              " 'unloose': 12023,\n",
              " 'generous': 8527,\n",
              " '##leterre': 12709,\n",
              " 'enemies': 2611,\n",
              " '##andise': 9850,\n",
              " 'mockeries': 19324,\n",
              " 'multiplied': 19853,\n",
              " 'cobbler': 18782,\n",
              " '##rand': 5623,\n",
              " 'losses': 7433,\n",
              " 'lurks': 18899,\n",
              " '##icero': 5994,\n",
              " 'deepest': 13083,\n",
              " 'soldier': 1089,\n",
              " 'penetrable': 17846,\n",
              " 'tying': 12948,\n",
              " 'coun': 502,\n",
              " '##aster': 2699,\n",
              " '##iol': 641,\n",
              " 'nine': 3674,\n",
              " 'lance': 6894,\n",
              " 'dispers': 7533,\n",
              " 'dreadful': 3665,\n",
              " 'preparation': 5330,\n",
              " 'decayed': 13497,\n",
              " '##thread': 16466,\n",
              " 'rooted': 8616,\n",
              " 'procule': 4134,\n",
              " 'obt': 6014,\n",
              " 'praising': 8518,\n",
              " 'kill': 1156,\n",
              " 'tetter': 16238,\n",
              " 'hotly': 12821,\n",
              " 'hazard': 3894,\n",
              " 'suggested': 13479,\n",
              " 'ga': 994,\n",
              " 'decreed': 9050,\n",
              " '##ght': 191,\n",
              " '##blood': 7498,\n",
              " 'minding': 12899,\n",
              " 'ruminate': 10260,\n",
              " 'mortified': 12936,\n",
              " 'resisted': 13504,\n",
              " 'robbery': 10898,\n",
              " 'swerve': 13714,\n",
              " 'mulier': 15623,\n",
              " 'blazon': 9663,\n",
              " 'unhand': 15075,\n",
              " 'runaways': 15846,\n",
              " 'famine': 9524,\n",
              " '##oons': 9224,\n",
              " 'uphold': 9320,\n",
              " 'jes': 6329,\n",
              " '##stairs': 17058,\n",
              " 'whistling': 13522,\n",
              " 'inaccessible': 19760,\n",
              " '##0': 82,\n",
              " 'ring': 1928,\n",
              " 'reconcil': 13524,\n",
              " 'catched': 18095,\n",
              " 'grec': 4366,\n",
              " 'egeus': 3836,\n",
              " 'courser': 11659,\n",
              " 'diomed': 1830,\n",
              " 'injur': 5044,\n",
              " 'undertakings': 15184,\n",
              " 'foil': 7766,\n",
              " 'noon': 6515,\n",
              " 'unconsider': 18243,\n",
              " 'holland': 6136,\n",
              " 'address': 6620,\n",
              " 'char': 608,\n",
              " 'convenient': 5585,\n",
              " 'bourde': 9621,\n",
              " 'hel': 424,\n",
              " 'condol': 14858,\n",
              " 'arrives': 19103,\n",
              " 'overtake': 10033,\n",
              " 'hopes': 3832,\n",
              " 'oliv': 1201,\n",
              " 'excepting': 13258,\n",
              " 'clustering': 14283,\n",
              " 'sharply': 13197,\n",
              " 'exit': 567,\n",
              " 'moralize': 15384,\n",
              " 'dinners': 17999,\n",
              " 'cri': 2259,\n",
              " 'brightly': 18193,\n",
              " 'froze': 18853,\n",
              " '##anied': 9815,\n",
              " 'sanctuary': 7913,\n",
              " 'proclamation': 6068,\n",
              " 'deaths': 5914,\n",
              " 'fifteen': 7066,\n",
              " 'comma': 17185,\n",
              " 'pac': 12733,\n",
              " 'gramercy': 12257,\n",
              " 'unfeigned': 13669,\n",
              " 'alph': 14374,\n",
              " 'elected': 11873,\n",
              " 'falsta': 478,\n",
              " 'solinus': 2640,\n",
              " 'chal': 9308,\n",
              " '##ability': 9948,\n",
              " 'luxury': 10336,\n",
              " '##oun': 391,\n",
              " 'tempt': 3591,\n",
              " 'pined': 12415,\n",
              " 'startles': 15194,\n",
              " 'mirrors': 17616,\n",
              " 'hired': 9196,\n",
              " 'knocked': 11887,\n",
              " 'cliff': 2207,\n",
              " 'somewh': 5644,\n",
              " 'pale': 2299,\n",
              " 'awful': 11603,\n",
              " 'posterior': 19645,\n",
              " 'gambols': 15906,\n",
              " 'covering': 10194,\n",
              " 'witches': 11971,\n",
              " 'truths': 11744,\n",
              " 'idly': 7300,\n",
              " 'conver': 8412,\n",
              " '##clip': 9417,\n",
              " '##ood': 208,\n",
              " 'confident': 6397,\n",
              " 'tast': 12432,\n",
              " 'furrow': 12999,\n",
              " 'woods': 7578,\n",
              " 'eloquent': 13568,\n",
              " 'ensh': 14238,\n",
              " '##hile': 2508,\n",
              " 'lammas': 14964,\n",
              " 'apparitions': 15105,\n",
              " 'altered': 15316,\n",
              " '##st': 109,\n",
              " '##ista': 1590,\n",
              " 'fab': 2174,\n",
              " 'cere': 3839,\n",
              " 'playing': 6246,\n",
              " 'induce': 9998,\n",
              " 'auspicious': 11121,\n",
              " 'scatter': 5286,\n",
              " 'dissolutely': 15849,\n",
              " 'bodykins': 17627,\n",
              " 'despising': 17854,\n",
              " 'reviving': 18634,\n",
              " 'presentment': 17438,\n",
              " 'lo': 217,\n",
              " 'forcing': 14163,\n",
              " 'graze': 9945,\n",
              " '##emen': 14307,\n",
              " 'sounded': 5105,\n",
              " 'sparingly': 16968,\n",
              " '##rates': 6344,\n",
              " 'planet': 8292,\n",
              " 'mock': 1944,\n",
              " 'hearkens': 17143,\n",
              " 'laws': 4711,\n",
              " 'descend': 5571,\n",
              " 'cowardice': 8203,\n",
              " 'fert': 6488,\n",
              " 'betrayed': 11976,\n",
              " 'lys': 1205,\n",
              " 'stricken': 14513,\n",
              " 'frenz': 7735,\n",
              " 'shrewd': 4521,\n",
              " 'ob': 1422,\n",
              " 'exc': 5179,\n",
              " 'latest': 7924,\n",
              " '##oubted': 9135,\n",
              " 'knotty': 15397,\n",
              " '##izing': 17078,\n",
              " 'demetrius': 1300,\n",
              " 'tab': 5263,\n",
              " 'withdrew': 16596,\n",
              " 'matched': 15033,\n",
              " 'fewest': 18246,\n",
              " 'requite': 5814,\n",
              " 'curbed': 17460,\n",
              " 'tried': 6093,\n",
              " 'affecting': 14914,\n",
              " 'serv': 447,\n",
              " '##anish': 8367,\n",
              " 'all': 222,\n",
              " 'superstitiously': 19339,\n",
              " '##arison': 5883,\n",
              " 'paddling': 18425,\n",
              " 'cent': 5251,\n",
              " 'humanity': 8644,\n",
              " 'lysander': 1574,\n",
              " 'ini': 8377,\n",
              " 'var': 3801,\n",
              " '##blest': 8910,\n",
              " 'urgent': 17828,\n",
              " 'scorch': 19374,\n",
              " 'lent': 4665,\n",
              " 'ord': 4418,\n",
              " 'eve': 8720,\n",
              " 'alas': 1669,\n",
              " 'today': 9819,\n",
              " 'ambush': 10855,\n",
              " 'homage': 7080,\n",
              " 'sedition': 18762,\n",
              " '##ertes': 1472,\n",
              " '##amis': 12567,\n",
              " 'dashed': 18842,\n",
              " 'devours': 14682,\n",
              " 'brides': 16846,\n",
              " 'dwelt': 19015,\n",
              " 'suffolk': 707,\n",
              " 'chim': 7744,\n",
              " 'wanteth': 13001,\n",
              " 'pound': 3754,\n",
              " 'os': 3507,\n",
              " 'determ': 3639,\n",
              " 'syllable': 8685,\n",
              " 'pining': 15051,\n",
              " 'barthol': 17881,\n",
              " 'caitiff': 7440,\n",
              " '##eck': 14031,\n",
              " 'handicraft': 17101,\n",
              " 'rests': 8139,\n",
              " '##ring': 2388,\n",
              " 'caparison': 11660,\n",
              " 'austerity': 15700,\n",
              " 'pallas': 15778,\n",
              " 'knack': 12631,\n",
              " 'thicker': 15299,\n",
              " 'recovery': 8615,\n",
              " '##quainted': 14482,\n",
              " 'climate': 9074,\n",
              " 'maiden': 3221,\n",
              " 'powder': 7993,\n",
              " '##vil': 14041,\n",
              " 'roe': 14485,\n",
              " 'stirring': 6428,\n",
              " 'inestimable': 16014,\n",
              " 'bil': 11262,\n",
              " '##ort': 277,\n",
              " 'web': 8394,\n",
              " '##imm': 14166,\n",
              " '##culapius': 18294,\n",
              " 'antig': 2322,\n",
              " 'omit': 5865,\n",
              " 'grained': 14476,\n",
              " 'gathered': 18539,\n",
              " 'disloyalty': 19012,\n",
              " 'promotion': 12993,\n",
              " 'envenomed': 19242,\n",
              " '-': 6,\n",
              " '##ru': 1441,\n",
              " 'affairs': 3557,\n",
              " 'plains': 5950,\n",
              " 'forgets': 14792,\n",
              " 'street': 2972,\n",
              " 'forward': 2830,\n",
              " '##icers': 16631,\n",
              " 'scion': 16974,\n",
              " 'encl': 16689,\n",
              " 'pertain': 10616,\n",
              " '##imo': 958,\n",
              " 'kine': 13922,\n",
              " 'prentice': 13736,\n",
              " 'locking': 15265,\n",
              " 'munition': 19438,\n",
              " 'shovel': 15874,\n",
              " 'scarce': 2804,\n",
              " 'gentilhomme': 19799,\n",
              " 'posts': 10141,\n",
              " 'desdem': 963,\n",
              " 'touching': 5533,\n",
              " 'petitions': 10919,\n",
              " 'complots': 15796,\n",
              " 'indued': 14628,\n",
              " 'raising': 10372,\n",
              " 'element': 7863,\n",
              " 'fog': 10620,\n",
              " 'always': 3610,\n",
              " 'misconst': 12263,\n",
              " '##ior': 1239,\n",
              " '##ican': 14198,\n",
              " 'playfellows': 14618,\n",
              " 'spruce': 16034,\n",
              " 'foemen': 16959,\n",
              " 'dungy': 19537,\n",
              " 'ice': 6010,\n",
              " 'thr': 986,\n",
              " 'gaultier': 19828,\n",
              " 'surly': 8939,\n",
              " 'forgiven': 13590,\n",
              " 'terrestri': 18715,\n",
              " 'rescue': 5339,\n",
              " 'censor': 18437,\n",
              " 'puffing': 18992,\n",
              " 'taxing': 18681,\n",
              " 'proclaim': 3008,\n",
              " 'century': 18662,\n",
              " 'testament': 8261,\n",
              " 'hardiment': 14836,\n",
              " 'rational': 19734,\n",
              " 'curbs': 12904,\n",
              " 'forbearance': 10971,\n",
              " 'islanders': 12035,\n",
              " '##oric': 8767,\n",
              " 'rightly': 5800,\n",
              " 'withheld': 13819,\n",
              " 'handling': 14490,\n",
              " 'cherish': 5334,\n",
              " 'gape': 8921,\n",
              " '##ieged': 11530,\n",
              " 'bray': 12659,\n",
              " 'husbands': 5311,\n",
              " 'ger': 12365,\n",
              " 'safer': 7044,\n",
              " 'monst': 1993,\n",
              " 'barm': 17876,\n",
              " 'shear': 12649,\n",
              " '##and': 181,\n",
              " 'dit': 7361,\n",
              " '##ustering': 10504,\n",
              " 'fidele': 8707,\n",
              " 'oce': 5468,\n",
              " 'execut': 2587,\n",
              " 'drives': 7568,\n",
              " 'am': 266,\n",
              " 'turned': 4706,\n",
              " 'toy': 7403,\n",
              " 'burgundy': 1901,\n",
              " 'knighthood': 9718,\n",
              " 'consuming': 11087,\n",
              " 'statesman': 18928,\n",
              " '##mark': 5613,\n",
              " '##fect': 2090,\n",
              " 'spacious': 8089,\n",
              " 'boon': 8430,\n",
              " 'addrest': 17921,\n",
              " 'players': 4772,\n",
              " 'edict': 8465,\n",
              " 'disgraces': 12303,\n",
              " 'unassail': 16896,\n",
              " 'performance': 6833,\n",
              " 'eggs': 10279,\n",
              " 'cramm': 12173,\n",
              " 'meca': 3897,\n",
              " 'vit': 13987,\n",
              " 'lipp': 14225,\n",
              " 'unchaste': 13647,\n",
              " 'corres': 14451,\n",
              " '--': 582,\n",
              " 'slaughters': 13273,\n",
              " 'romans': 3669,\n",
              " '##vert': 5759,\n",
              " '##oult': 2567,\n",
              " 'leisurely': 18292,\n",
              " 'wrink': 5522,\n",
              " '##are': 434,\n",
              " 'quartered': 18627,\n",
              " 'glasses': 9566,\n",
              " 'continent': 8591,\n",
              " 'corrig': 17047,\n",
              " 'vouchsaf': 18327,\n",
              " 'disclaim': 9943,\n",
              " 'brotherhood': 10691,\n",
              " 'pester': 12411,\n",
              " '##essant': 12562,\n",
              " 'woodman': 15043,\n",
              " 'mounteb': 11860,\n",
              " 'nails': 5059,\n",
              " 'dealt': 9296,\n",
              " 'thisby': 4990,\n",
              " '##igm': 10633,\n",
              " 'unfinish': 15094,\n",
              " 'pat': 946,\n",
              " 'busily': 17520,\n",
              " '##eart': 11363,\n",
              " 'plated': 16925,\n",
              " 'archdeacon': 19845,\n",
              " 'wight': 8372,\n",
              " '##ath': 176,\n",
              " 'intimate': 14815,\n",
              " 'impatiently': 15529,\n",
              " 'adultery': 10305,\n",
              " '##inado': 14075,\n",
              " '##blin': 14666,\n",
              " 'unlawful': 8305,\n",
              " 'caus': 9891,\n",
              " 'wearied': 10049,\n",
              " 'ptolemy': 15866,\n",
              " 'expedient': 9086,\n",
              " 'defiance': 8180,\n",
              " 'broom': 14554,\n",
              " 'hymns': 18341,\n",
              " 'ercast': 17183,\n",
              " '##maker': 14059,\n",
              " 'woo': 1929,\n",
              " 'merc': 3773,\n",
              " 'tot': 11400,\n",
              " 'gul': 7979,\n",
              " 'brewing': 19074,\n",
              " 'alarum': 2737,\n",
              " 'reprehend': 13718,\n",
              " 'follow': 974,\n",
              " 'particular': 3631,\n",
              " 'haggard': 13734,\n",
              " 'anatomize': 19525,\n",
              " 'experienced': 12056,\n",
              " '##olier': 16538,\n",
              " 'brit': 3028,\n",
              " 'roasted': 9367,\n",
              " 'bosworth': 17843,\n",
              " 'brands': 9127,\n",
              " 'unpr': 16884,\n",
              " 'cicero': 6006,\n",
              " 'distinctly': 10363,\n",
              " 'appar': 2936,\n",
              " 'variable': 11001,\n",
              " 'navarre': 8293,\n",
              " 'instances': 10783,\n",
              " '##itably': 16556,\n",
              " 'lute': 6333,\n",
              " 'consangu': 17550,\n",
              " 'drown': 2566,\n",
              " 'infernal': 17713,\n",
              " 'fairness': 12796,\n",
              " 'rigorous': 13542,\n",
              " 'resembling': 18887,\n",
              " 'sciences': 16984,\n",
              " 'concave': 18225,\n",
              " 'belzebub': 19811,\n",
              " 'slip': 5787,\n",
              " 'stool': 6364,\n",
              " 'drive': 4445,\n",
              " 'messala': 3306,\n",
              " '##gging': 11880,\n",
              " 'carved': 10803,\n",
              " '##osh': 11498,\n",
              " 'conveniences': 18574,\n",
              " '##cock': 7141,\n",
              " 'yearn': 13181,\n",
              " '##know': 14044,\n",
              " '##ivals': 17024,\n",
              " 'waft': 8570,\n",
              " 'pract': 2676,\n",
              " 'conceited': 10164,\n",
              " 'suprem': 7918,\n",
              " 'yielded': 5678,\n",
              " 'infants': 8539,\n",
              " 'hest': 11294,\n",
              " 'rout': 7995,\n",
              " 'reprieve': 11223,\n",
              " 'childishness': 15674,\n",
              " 'grandmother': 13141,\n",
              " '##ilius': 1598,\n",
              " '##icorn': 14199,\n",
              " 'britain': 4520,\n",
              " 'unbr': 9912,\n",
              " 'gentleman': 500,\n",
              " 'lion': 2267,\n",
              " 'pendent': 13732,\n",
              " 'flattery': 5817,\n",
              " '##vir': 1886,\n",
              " 'bosoms': 5953,\n",
              " '##itches': 14150,\n",
              " '##lestick': 15735,\n",
              " 'showering': 19031,\n",
              " 'extol': 11808,\n",
              " 'pennyworth': 11066,\n",
              " 'reverberate': 17919,\n",
              " 'sighted': 14886,\n",
              " 'self': 2433,\n",
              " '##pros': 12491,\n",
              " '##bread': 16375,\n",
              " 'warranty': 17749,\n",
              " 'feigned': 10298,\n",
              " 'unshaked': 19041,\n",
              " 'cedar': 9157,\n",
              " '##mund': 1076,\n",
              " 'incline': 9069,\n",
              " 'security': 9356,\n",
              " 'lengthen': 10221,\n",
              " '##go': 3848,\n",
              " 'sender': 12426,\n",
              " 'cassibelan': 13833,\n",
              " 'pisanio': 1448,\n",
              " 'cond': 1667,\n",
              " '##clusive': 14967,\n",
              " 'plunged': 15750,\n",
              " 'stretches': 18532,\n",
              " '##reement': 12534,\n",
              " '##ween': 1587,\n",
              " 'vel': 6021,\n",
              " 'starts': 7898,\n",
              " '##anor': 6513,\n",
              " 'budge': 7971,\n",
              " 'dish': 2191,\n",
              " '##raphy': 16647,\n",
              " 'rightful': 7519,\n",
              " 'defe': 6211,\n",
              " 'circumstance': 4599,\n",
              " \",--'\": 11628,\n",
              " 'largely': 15146,\n",
              " 'dimension': 18518,\n",
              " 'cavaliers': 19676,\n",
              " 'lazy': 8072,\n",
              " 'haunts': 9627,\n",
              " 'clearness': 18152,\n",
              " 'evermore': 5646,\n",
              " 'miscall': 18637,\n",
              " 'hyperbolical': 19122,\n",
              " 'chief': 3810,\n",
              " 'prevent': 3830,\n",
              " 'process': 5517,\n",
              " 'reven': 1607,\n",
              " 'expected': 8599,\n",
              " 'sne': 9212,\n",
              " 'revenging': 17658,\n",
              " 'romeo': 649,\n",
              " 'icicles': 15731,\n",
              " 'hovel': 9387,\n",
              " 'closed': 10568,\n",
              " 'furniture': 12003,\n",
              " 'assigns': 19158,\n",
              " 'rendez': 13476,\n",
              " 'divide': 5559,\n",
              " '##rious': 9875,\n",
              " 'dukedoms': 9649,\n",
              " '##iving': 11572,\n",
              " 'unwillingly': 11242,\n",
              " 'mar': 221,\n",
              " 'bribed': 16070,\n",
              " 'hange': 16122,\n",
              " 'conformable': 19821,\n",
              " 'monde': 19149,\n",
              " '##bate': 16370,\n",
              " 'unnatural': 4629,\n",
              " 'guided': 11646,\n",
              " '##der': 689,\n",
              " 'whirlwinds': 19420,\n",
              " 'writer': 17820,\n",
              " 'pense': 10862,\n",
              " 'cub': 12344,\n",
              " 'rendezvous': 13840,\n",
              " 'lentus': 18566,\n",
              " 'camp': 3134,\n",
              " 'thames': 9804,\n",
              " '##ee': 1037,\n",
              " 'cont': 757,\n",
              " 'dolabella': 3305,\n",
              " 'village': 10735,\n",
              " 'outliv': 11562,\n",
              " 'wolvish': 14657,\n",
              " 'reason': 1184,\n",
              " 'creaking': 18081,\n",
              " 'deriv': 14951,\n",
              " 'absolute': 5035,\n",
              " 'tilth': 19025,\n",
              " 'mine': 520,\n",
              " 'prevented': 6145,\n",
              " '##quess': 8460,\n",
              " 'alphab': 19586,\n",
              " 'allay': 6727,\n",
              " 'benef': 3694,\n",
              " 'dishon': 2869,\n",
              " 'private': 3095,\n",
              " 'remembr': 3353,\n",
              " 'duty': 2075,\n",
              " 'cote': 11269,\n",
              " 'surmount': 19078,\n",
              " 'eight': 4039,\n",
              " 'obtaining': 15531,\n",
              " 'pursuivant': 10367,\n",
              " 'lath': 11301,\n",
              " 'featly': 19047,\n",
              " 'foppery': 12715,\n",
              " 'narrow': 5888,\n",
              " 'befriended': 19215,\n",
              " 'stalls': 15713,\n",
              " '##eys': 7390,\n",
              " 'puritan': 10009,\n",
              " 'twin': 8456,\n",
              " 'link': 9752,\n",
              " 'unpeopled': 15749,\n",
              " 'servingman': 2284,\n",
              " 'boundless': 10815,\n",
              " 'pandulph': 1900,\n",
              " 'fierceness': 15992,\n",
              " 'spurns': 11064,\n",
              " '##estial': 6536,\n",
              " 'blamed': 11533,\n",
              " '##nus': 14008,\n",
              " '##bre': 14051,\n",
              " 'compassing': 15239,\n",
              " 'grunt': 17084,\n",
              " 'oaths': 3458,\n",
              " 'bennet': 17241,\n",
              " 'prisoners': 4216,\n",
              " 'spurio': 18544,\n",
              " 'pewter': 19482,\n",
              " 'capric': 17270,\n",
              " 'superficial': 19517,\n",
              " 'filth': 5450,\n",
              " 'withhold': 8036,\n",
              " '##flow': 9079,\n",
              " '##alion': 12535,\n",
              " 'branches': 6861,\n",
              " '##lemain': 16562,\n",
              " 'trip': 6778,\n",
              " 'squint': 18276,\n",
              " 'girdled': 19163,\n",
              " 'fleeting': 15453,\n",
              " 'salique': 8889,\n",
              " 'curtsy': 9701,\n",
              " 'upli': 11513,\n",
              " 'beating': 5406,\n",
              " 'height': 4677,\n",
              " 'dishonest': 7585,\n",
              " 'aggra': 14401,\n",
              " 'wasting': 15090,\n",
              " 'prompter': 17635,\n",
              " '##enerate': 8161,\n",
              " '##vers': 1364,\n",
              " 'poten': 10450,\n",
              " '##aking': 1874,\n",
              " 'conqu': 2687,\n",
              " 'tread': 4123,\n",
              " 'remains': 4723,\n",
              " 'infirmity': 7086,\n",
              " \",'\": 1230,\n",
              " '##ister': 1260,\n",
              " '##usual': 11398,\n",
              " 'wearies': 17542,\n",
              " 'col': 1902,\n",
              " '##pared': 12492,\n",
              " 'consume': 8943,\n",
              " '##cription': 11379,\n",
              " 'sampler': 19736,\n",
              " 'admonition': 17949,\n",
              " 'approof': 12875,\n",
              " 'forerunner': 15815,\n",
              " 'revenge': 2114,\n",
              " '##iction': 6109,\n",
              " 'bridegroom': 6882,\n",
              " 'swaggering': 13689,\n",
              " '##cious': 1410,\n",
              " '##enius': 744,\n",
              " 'parado': 11612,\n",
              " 'limp': 13151,\n",
              " 'fled': 3107,\n",
              " 'minister': 3355,\n",
              " 'shotten': 14145,\n",
              " 'incred': 17774,\n",
              " 'drachmas': 19620,\n",
              " 'offspring': 12324,\n",
              " 'failing': 9567,\n",
              " 'assault': 6098,\n",
              " 'level': 6369,\n",
              " 'unlike': 6738,\n",
              " 'cott': 9187,\n",
              " 'prisons': 13029,\n",
              " 'philippe': 17667,\n",
              " 'prettily': 12170,\n",
              " 'tawny': 7548,\n",
              " 'flaming': 12005,\n",
              " '##ado': 1280,\n",
              " 'dank': 10421,\n",
              " '##iency': 9784,\n",
              " 'insurre': 13210,\n",
              " 'centa': 8659,\n",
              " 'stare': 8040,\n",
              " 'gramer': 9366,\n",
              " 'essex': 13254,\n",
              " 'frost': 5625,\n",
              " 'slightest': 15311,\n",
              " '##ium': 8350,\n",
              " '##ilet': 14228,\n",
              " 'observing': 11922,\n",
              " '##ourt': 9873,\n",
              " 'stone': 3211,\n",
              " 'foulest': 17661,\n",
              " '##pter': 8763,\n",
              " 'eternity': 10240,\n",
              " 'communication': 19260,\n",
              " 'encounters': 10943,\n",
              " 'almanac': 16030,\n",
              " '##reet': 7152,\n",
              " 'divinity': 8682,\n",
              " 'hacket': 18902,\n",
              " 'waste': 4090,\n",
              " 'beholding': 5026,\n",
              " 'tyrants': 9012,\n",
              " 'turret': 18167,\n",
              " 'aiming': 18364,\n",
              " 'tallow': 10461,\n",
              " 'adverse': 7767,\n",
              " 'perhaps': 4061,\n",
              " 'capacity': 8118,\n",
              " 'noun': 13939,\n",
              " 'sadly': 7250,\n",
              " '##regn': 16530,\n",
              " 'stopp': 6431,\n",
              " 'profitably': 18338,\n",
              " 'eag': 4085,\n",
              " 'pertinent': 19068,\n",
              " 'harrow': 19542,\n",
              " 'ongles': 16656,\n",
              " 'splendor': 18531,\n",
              " 'battering': 18757,\n",
              " '##olas': 8027,\n",
              " 'credo': 13200,\n",
              " 'ithaca': 19716,\n",
              " 'have': 177,\n",
              " 'votaries': 18767,\n",
              " 'liegemen': 17893,\n",
              " 'vessels': 13372,\n",
              " '##ivers': 5877,\n",
              " 'impos': 14729,\n",
              " '##emont': 16800,\n",
              " 'pilch': 18410,\n",
              " 'portent': 12814,\n",
              " 'pitiful': 5219,\n",
              " 'history': 6472,\n",
              " 'graver': 10650,\n",
              " 'this': 167,\n",
              " 'acquaint': 3628,\n",
              " 'amity': 6222,\n",
              " 'purses': 7805,\n",
              " 'imm': 2754,\n",
              " 'continuate': 18105,\n",
              " '##ensible': 11611,\n",
              " 'pennyworths': 18609,\n",
              " 'lawy': 6262,\n",
              " 'pip': 7990,\n",
              " 'slaughtermen': 18229,\n",
              " 'benefits': 10203,\n",
              " 'vision': 6806,\n",
              " 'pollution': 19071,\n",
              " 'bids': 3684,\n",
              " 'aguecheek': 16055,\n",
              " 'helmet': 10266,\n",
              " 'ballad': 6462,\n",
              " '##yond': 3602,\n",
              " 'fillip': 18002,\n",
              " 'oxen': 9613,\n",
              " 'eyel': 8103,\n",
              " 'firebrand': 17618,\n",
              " 'sulphurous': 10352,\n",
              " '##isted': 12661,\n",
              " 'earliest': 14483,\n",
              " 'candied': 15425,\n",
              " 'barbara': 18613,\n",
              " 'dew': 4053,\n",
              " 'athen': 3494,\n",
              " 'unn': 11523,\n",
              " 'calculate': 17444,\n",
              " 'cranny': 17823,\n",
              " 'alacrity': 13825,\n",
              " '##lier': 9230,\n",
              " 'offered': 7264,\n",
              " 'relief': 8213,\n",
              " 'rave': 11328,\n",
              " '##fle': 8757,\n",
              " 'essent': 13661,\n",
              " 'sardis': 13765,\n",
              " 'bode': 10178,\n",
              " 'arms': 1514,\n",
              " 'prolong': 10915,\n",
              " '##atham': 16651,\n",
              " 'smelling': 10970,\n",
              " 'inclusive': 16520,\n",
              " 'apric': 17422,\n",
              " 'eaning': 19538,\n",
              " '##is': 97,\n",
              " 'appro': 2032,\n",
              " 'slept': 4850,\n",
              " 'grease': 10617,\n",
              " 'longest': 10687,\n",
              " 'deux': 16668,\n",
              " '##genorth': 18750,\n",
              " 'dissever': 14462,\n",
              " 'fond': 3230,\n",
              " 'quickening': 17357,\n",
              " 'merciless': 13835,\n",
              " '##bolton': 18879,\n",
              " 'cum': 12343,\n",
              " 'hasty': 6566,\n",
              " 'vienna': 8435,\n",
              " 'greatly': 8104,\n",
              " 'thie': 3704,\n",
              " 'tith': 13981,\n",
              " 'gift': 2523,\n",
              " 'supervise': 18444,\n",
              " 'companion': 4326,\n",
              " 'well': 316,\n",
              " 'squire': 7057,\n",
              " 'appearance': 8549,\n",
              " 'jarring': 13803,\n",
              " 'dreamed': 17723,\n",
              " 'primroses': 19034,\n",
              " 'accoutred': 19319,\n",
              " 'mickle': 11314,\n",
              " 'halloo': 13297,\n",
              " 'rive': 9764,\n",
              " 'oath': 1780,\n",
              " '##face': 9786,\n",
              " '##at': 90,\n",
              " 'burg': 1863,\n",
              " '##ands': 2394,\n",
              " 'jell': 13911,\n",
              " 'rainy': 15163,\n",
              " '##iot': 4412,\n",
              " 'cozened': 8649,\n",
              " '##ines': 2375,\n",
              " 'kin': 4664,\n",
              " 'approve': 5815,\n",
              " \"--'\": 10666,\n",
              " 'inferior': 9670,\n",
              " 'smiling': 5122,\n",
              " '##iors': 5479,\n",
              " 'appertaining': 15801,\n",
              " 'mandragora': 19439,\n",
              " 'dwells': 6450,\n",
              " '##ienced': 10280,\n",
              " 'ss': 16207,\n",
              " 'kath': 627,\n",
              " 'squ': 3468,\n",
              " '##thumus': 902,\n",
              " '##agree': 16859,\n",
              " 'unpl': 18196,\n",
              " 'cygnet': 19663,\n",
              " '##ifying': 11748,\n",
              " 'bridle': 13501,\n",
              " '##ocks': 12916,\n",
              " 'prettiest': 9138,\n",
              " 'thereabouts': 12648,\n",
              " 'ambiguities': 19641,\n",
              " '##amental': 18296,\n",
              " '##oy': 3541,\n",
              " '##any': 1002,\n",
              " 'beasts': 4648,\n",
              " 'eft': 16092,\n",
              " 'charters': 14555,\n",
              " '##aks': 11511,\n",
              " 'bellario': 9017,\n",
              " 'ripen': 9666,\n",
              " 'machiavel': 19355,\n",
              " 'gather': 4537,\n",
              " 'whoever': 8436,\n",
              " 'rheumatic': 15545,\n",
              " 'giglot': 15816,\n",
              " '##elius': 2882,\n",
              " 'children': 2478,\n",
              " 'beaum': 14829,\n",
              " 'chick': 9867,\n",
              " 'arden': 10584,\n",
              " 'populous': 11069,\n",
              " 'vineyard': 12161,\n",
              " 'busy': 5539,\n",
              " 'hopeful': 9439,\n",
              " 'ranker': 15200,\n",
              " '##ffed': 16757,\n",
              " '##antages': 8059,\n",
              " 'autolycus': 1326,\n",
              " 'beds': 5658,\n",
              " 'leviath': 15440,\n",
              " 'bladders': 15939,\n",
              " 'seizing': 19300,\n",
              " '##ucius': 7393,\n",
              " '##chan': 8373,\n",
              " 'labour': 2351,\n",
              " '##teth': 11352,\n",
              " 'kil': 13921,\n",
              " 'fume': 13889,\n",
              " 'circling': 18080,\n",
              " '##imonies': 14949,\n",
              " 'oration': 8817,\n",
              " 'yew': 11696,\n",
              " 'baron': 9724,\n",
              " '##waul': 16398,\n",
              " 'officer': 1962,\n",
              " 'gage': 6676,\n",
              " 'lis': 13924,\n",
              " 'gambol': 12110,\n",
              " 'shamest': 13618,\n",
              " 'creeps': 13387,\n",
              " 'rob': 2493,\n",
              " 'milkmaid': 18577,\n",
              " '##imes': 2491,\n",
              " 'gnawn': 16119,\n",
              " 'revolving': 18179,\n",
              " 'silence': 3135,\n",
              " 'gobl': 9880,\n",
              " 'shafalus': 19763,\n",
              " 'rochester': 14487,\n",
              " '##ately': 16937,\n",
              " 'mud': 5862,\n",
              " '##ope': 4672,\n",
              " 'elect': 13144,\n",
              " 'seditious': 18763,\n",
              " 'terrestrial': 19854,\n",
              " 'voices': 3660,\n",
              " 'thus': 686,\n",
              " 'saf': 2838,\n",
              " 'evilly': 17454,\n",
              " 'servitors': 14466,\n",
              " 'tickle': 8657,\n",
              " 'bloodsh': 17228,\n",
              " 'win': 1251,\n",
              " 'ghastly': 11789,\n",
              " 'bathed': 13566,\n",
              " 'presc': 7486,\n",
              " 'ravens': 7592,\n",
              " 'spain': 8087,\n",
              " 'skies': 10087,\n",
              " 'dislike': 6555,\n",
              " 'disable': 17066,\n",
              " 'camps': 11680,\n",
              " 'activity': 14679,\n",
              " 'aumerle': 2371,\n",
              " 'strokes': 6469,\n",
              " '##once': 7718,\n",
              " 'training': 15086,\n",
              " 'dives': 14898,\n",
              " '##ades': 1606,\n",
              " 'obscene': 12092,\n",
              " 'grinning': 15755,\n",
              " '##dock': 16336,\n",
              " 'quarre': 8995,\n",
              " 'insc': 9264,\n",
              " 'poor': 797,\n",
              " 'folk': 6888,\n",
              " 'sirs': 4101,\n",
              " '##take': 7131,\n",
              " 'brim': 7753,\n",
              " 'had': 418,\n",
              " 'speediest': 14758,\n",
              " 'commanded': 4588,\n",
              " 'misc': 5103,\n",
              " 'imprison': 4959,\n",
              " 'besides': 2497,\n",
              " 'carriages': 10116,\n",
              " 'prudent': 13737,\n",
              " 'erebus': 17522,\n",
              " 'morrow': 1276,\n",
              " 'locks': 6943,\n",
              " 'swashing': 19588,\n",
              " 'precisely': 12235,\n",
              " 'appertain': 10013,\n",
              " '##ented': 9276,\n",
              " '##ree': 642,\n",
              " '##arr': 154,\n",
              " ...}"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create tokenizer and vocabulary\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "print(f\"The vocabulary size is {vocab_size}.\")\n",
        "\n",
        "# # uncomment to see the long list of vocabulary and corresponding integer IDs\n",
        "tokenizer.get_vocab()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IQP89r8_aXb"
      },
      "source": [
        "### Tokenize dataset\n",
        "* Use the tokenizer and vocabulary to convert the dataset into a sequence of integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPrLEjvB_aXb"
      },
      "outputs": [],
      "source": [
        "def data_process(\n",
        "    data_list: List[str],\n",
        "    tokenizer: Callable,\n",
        ") -> Tensor:\n",
        "    \"\"\"Converts raw text into a flat Tensor.\n",
        "\n",
        "    Args:\n",
        "        data_list (List[str]): A list of raw text data.\n",
        "        tokenizer (Callable): An English tokenizer.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: A flat Tensor containing the numeric form of the flatten text.\n",
        "    \"\"\"\n",
        "    data = [torch.tensor(tokenizer.encode(item).ids, dtype=torch.long)\n",
        "            for item in data_list]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9k3yURX_aXb",
        "outputId": "8a7c7393-be87-4ce7-f154-13b72e01f399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick look at the numericalized data tensor([ 3197,   775,    18,  ...,    18,   543, 19863]).\n",
            "The length of the training_data is 1338875.\n",
            "The length of the validation_data is 79221.\n"
          ]
        }
      ],
      "source": [
        "# Here we convert the raw training and validation data\n",
        "# into tensors of token IDs.\n",
        "training_data = data_process(training_list, tokenizer)\n",
        "validation_data = data_process(validation_list, tokenizer)\n",
        "print(f'Quick look at the numericalized data {training_data}.')\n",
        "print(f'The length of the training_data is {len(training_data)}.')\n",
        "print(f'The length of the validation_data is {len(validation_data)}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqmN0C-h_aXb"
      },
      "source": [
        "## Batchify dataset\n",
        "- Create batches out of the long list of number\n",
        "- This allows for parallel computation during training\n",
        "- Drawback being the relationship between the batches are not learned\n",
        "\n",
        "See this link for more detail: https://pytorch.org/tutorials/beginner/transformer_tutorial.html#load-and-batch-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6BwMp4a_aXb"
      },
      "outputs": [],
      "source": [
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve-k02GH_aXb"
      },
      "source": [
        "## Generate input and target sequence\n",
        "- Grab an input sequence with length `seq_len` from the batchify data starting at index `i`\n",
        "- The target sequence also has the same length `seq_len` but the starting index is `i+1`\n",
        "\n",
        "See this link for more detail: https://pytorch.org/tutorials/beginner/transformer_tutorial.html#functions-to-generate-input-and-target-sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCsI7YVt_aXc"
      },
      "outputs": [],
      "source": [
        "def get_batch(source: Tensor, i: int, bptt: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZ9jlohq_aXc",
        "outputId": "e1723f9c-ae12-4096-d6fd-cb9b5e6bf9e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20919"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(batchify(training_data, 64))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpWBHq_c_aXc",
        "outputId": "b87fdbaa-a224-4021-9886-40b5aae34ec3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example data: tensor([[  350,    18,   175,   686,   169,  2235,  1356,   416,   115,  4006,\n",
            "         19863,   350,    18,   234,  6987,     5,  2927,     5, 13726,  9542,\n",
            "            19, 19863,  2868,    18,   102,  1047,   119,   222,   167,  2545,\n",
            "          2293,   160,   835,    19, 19863,   350,    18,   102,   256,     5,\n",
            "           130,   560,   414,  2863,   414,   835,     7, 19863,   350,    18,\n",
            "           343,   177,    30, 15119,   256],\n",
            "        [  104,   507,     7, 19863,  1633,    18,  2084,  5170,  1574,     5,\n",
            "           239,   295,  1083,    22,  6366,     1, 19863,   173,    18,  1627,\n",
            "           776,    99,  4068, 19863,  1581,    18,   180,    93,  1960, 19863,\n",
            "          1581,    18,  1240,  1371,    18, 19863,  1581,    18,    30,     2,\n",
            "           298,  6995, 19863,  1581,    18,   102,   160,  1096,     5, 19863,\n",
            "          1581,    18,   373,  3344,     5]], device='cuda:0')\n",
            "Example targets: tensor([   18,   507,   175,     7,   686, 19863,   169,  1633,  2235,    18,\n",
            "         1356,  2084,   416,  5170,   115,  1574,  4006,     5, 19863,   239,\n",
            "          350,   295,    18,  1083,   234,    22,  6987,  6366,     5,     1,\n",
            "         2927, 19863,     5,   173, 13726,    18,  9542,  1627,    19,   776,\n",
            "        19863,    99,  2868,  4068,    18, 19863,   102,  1581,  1047,    18,\n",
            "          119,   180,   222,    93,   167,  1960,  2545, 19863,  2293,  1581,\n",
            "          160,    18,   835,  1240,    19,  1371, 19863,    18,   350, 19863,\n",
            "           18,  1581,   102,    18,   256,    30,     5,     2,   130,   298,\n",
            "          560,  6995,   414, 19863,  2863,  1581,   414,    18,   835,   102,\n",
            "            7,   160, 19863,  1096,   350,     5,    18, 19863,   343,  1581,\n",
            "          177,    18,    30,   373, 15119,  3344,   256,     5,    19,  3588],\n",
            "       device='cuda:0')\n",
            "=== Example data[0] decoded ===\n",
            "gloucester : but thus his simple truth must be abused \n",
            " gloucester : by silken, sly, insinuating jacks? \n",
            " rivers : to whom in all this presence speaks your grace? \n",
            " gloucester : to thee, that hast nor honesty nor grace. \n",
            " gloucester : when have i injured thee\n",
            "=== Example data[1] decoded ===\n",
            "of day. \n",
            " hermia : heavens shield lysander, if they mean a fray! \n",
            " narrator : lies down and sleeps \n",
            " puck : on the ground \n",
            " puck : sleep sound : \n",
            " puck : i ' ll apply \n",
            " puck : to your eye, \n",
            " puck : gentle lover,\n",
            "=== Example targets decoded ===\n",
            ": day but. thus \n",
            " his hermia simple : truth heavens must shield be lysander abused, \n",
            " if\n",
            "''' play : richard iii ''' \n",
            " narrator : act i \n",
            " narrator : scene i. london. a street. \n",
            " narrator : enter gloucester, solus \n",
            " gloucester : now is the winter of our discontent \n",
            " gloucester : made glorious summer by this sun of york, \n",
            " gloucester : and all the clouds that lour ' d upon our house \n",
            " gloucester : in the deep bosom of the ocean buried. \n",
            " gloucester : now are our brows bound with victorious wreaths, \n",
            " gloucester : our bruised arms hung up for monuments\n"
          ]
        }
      ],
      "source": [
        "# Example batch drawn from the data and decoded\n",
        "# to show what the model will be predicting against\n",
        "def print_example_batch():\n",
        "    example_batch = batchify(training_data, 2)\n",
        "    example_data, example_targets = get_batch(example_batch, 6485, 55)\n",
        "    print(f\"Example data: {example_data.t()}\")\n",
        "    print(f\"Example targets: {example_targets}\")\n",
        "    print(\n",
        "        f\"=== Example data[0] decoded ===\\n{tokenizer.decode(example_data[:, 0].tolist())}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"=== Example data[1] decoded ===\\n{tokenizer.decode(example_data[:, 1].tolist())}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"=== Example targets decoded ===\\n{tokenizer.decode(example_targets.tolist()[:20])}\"\n",
        "    )\n",
        "\n",
        "\n",
        "print_example_batch()\n",
        "\n",
        "print(tokenizer.decode(batchify(training_data, 2).to(\"cpu\")[:, 0].tolist()[:100]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLmq0ItS_aXc"
      },
      "source": [
        "## Create recurrent neural network model\n",
        "A recurrent neural network with an embedding layer, one or more recurrent layers, and a linear layer.\n",
        "\n",
        "We use this `RecurrentModel` class to describe the behavior of the embedding and output layers, which are the same no matter which recurrent unit we use. We then pass the recurrent unit as a parameter to the `RecurrentModel` constructor to describe the behavior of the recurrent layers. This lets us easily create models with different recurrent units to compare their performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzKeWJ-H_aXc"
      },
      "outputs": [],
      "source": [
        "class RecurrentModel(Module):\n",
        "    \"\"\"The recurrent neural network.\"\"\"\n",
        "\n",
        "    # __init__ is the constructor.\n",
        "    # It sets up new instances of the class.\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        recurrent_module: Union[RNN, LSTM, GRU],\n",
        "    ) -> None:\n",
        "        \"\"\"Initialize the recurrent neural network.\n",
        "\n",
        "        Note: For this model architecure, if the number of layers is greater\n",
        "        than 1, then embedding size and hidden size must be equal.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): The number of vocabulary in the dataset.\n",
        "            embedding_dim (int): The dimension of the embedding output.\n",
        "            hidden_size (int): The size of the recurrent unit's hidden state.\n",
        "            num_layers (int): The number of recurrent unit layers.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the number of layers is greater than 1 and the\n",
        "            embedding size is not equal to the hidden size.\n",
        "        \"\"\"\n",
        "        # First, we call the superclass constructor\n",
        "        # to give PyTorch a chance to set up the\n",
        "        # parts of the object it needs to.\n",
        "        super().__init__()\n",
        "\n",
        "        # num_layers is how many layers the recurrent unit has.\n",
        "        num_layers = recurrent_module.num_layers\n",
        "\n",
        "        # hidden_size is the size of the hidden state of the recurrent unit.\n",
        "        hidden_size = recurrent_module.hidden_size\n",
        "\n",
        "        # embedding_dim is the size of the embedding output.\n",
        "        embedding_dim = recurrent_module.input_size\n",
        "\n",
        "        if (num_layers > 1) and (hidden_size != embedding_dim):\n",
        "            raise ValueError(\n",
        "                \"When the number of layers is greater than 1, the embedding \"\n",
        "                \"size and hidden size must be equal.\"\n",
        "            )\n",
        "\n",
        "        # The embedding layer turns the token IDs into embedding vectors.\n",
        "        # This is a matrix of size [vocab_size, embedding_dim].\n",
        "        # Each row of the matrix corresponds to one token in the vocabulary,\n",
        "        #  providing a sort of \"meaning in isolation\" for that token.\n",
        "        # The embedding layer is a trainable part of the model,\n",
        "        #  so it will be updated during training.\n",
        "        # The values start out initialized randomly.\n",
        "        self.embedding = Embedding(\n",
        "            num_embeddings=vocab_size, embedding_dim=embedding_dim\n",
        "        )\n",
        "\n",
        "        # The recurrent module is the main part of the model.\n",
        "        # It takes in a sequence of embedding vectors\n",
        "        #  and spreads information across the sequence.\n",
        "        self.rnn = recurrent_module\n",
        "\n",
        "        # The linear layer is the output layer of the model.\n",
        "        # It takes in the final embedding vector from the\n",
        "        #  recurrent module and outputs a vector of size [vocab_size].\n",
        "        # This vector contains a score for each token in the vocabulary.\n",
        "        # The token with the highest score is the one the model predicts\n",
        "        #  as the next token, so the model will try to make the score\n",
        "        #  for the correct token as high as possible.\n",
        "        self.linear = Linear(hidden_size, vocab_size)\n",
        "\n",
        "    # The forward method is called when we pass data through the model.\n",
        "    def forward(\n",
        "        self,\n",
        "        x: Tensor,\n",
        "        prev_state: Optional[Union[Tensor, Tuple[Tensor, Tensor]]],\n",
        "    ) -> Tuple[Tensor, Union[Tensor, Tuple[Tensor, Tensor]]]:\n",
        "        \"\"\"Pass the a batch of data through the recurrent neural network model\n",
        "        along with the previous state.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): The batch of data.\n",
        "            prev_states (Tensor): The previous states of the recurrent units.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[Tensor, Tensor]: The logits and hidden states.\n",
        "        \"\"\"\n",
        "\n",
        "        # The embedding layer turns the token IDs into embedding vectors.\n",
        "        # TODO: Embed the input tokens using the embedding layer.\n",
        "        embedded = self.embedding(x)\n",
        "        # The recurrent module takes in a sequence of embedding vectors\n",
        "        #  and spreads information across the sequence via its hidden state.\n",
        "        # TODO: Pass the embedded input tokens into the recurrent module along with the previous state.\n",
        "        output, state = self.rnn(embedded, prev_state)\n",
        "        # The linear layer takes in the final embedding vector from the\n",
        "        #  recurrent module and outputs a vector of size [vocab_size].\n",
        "        # TODO: Pass the final embedding vector into the linear layer to get the logits.\n",
        "        logits=self.linear(output)\n",
        "        # Return the logits and the final states.\n",
        "        # TODO: Return the logits and the final states.\n",
        "        return logits, state\n",
        "    def detach_state_(self, states: Union[Tensor, Tuple[Tensor, Tensor]]) -> None:\n",
        "        \"\"\"Detach the state of the recurrent units.\n",
        "\n",
        "        This function is used to make sure the model doesn't try to backpropagate\n",
        "        through the entire history of the sequence, as our computers can't\n",
        "        handle that much data.\n",
        "\n",
        "        Args:\n",
        "            states (Union[Tensor, Tuple[Tensor, Tensor]]): The states of the\n",
        "            recurrent units.\n",
        "        \"\"\"\n",
        "        if isinstance(states, Tensor):\n",
        "            states.detach_()\n",
        "        elif isinstance(states, tuple):\n",
        "            for state in states:\n",
        "                state.detach_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9kpRgKF_aXc"
      },
      "source": [
        "## Define training loss\n",
        "The loss function is the negative log likelihood loss since we are doing classification.\n",
        "\n",
        "The classification problem is, based on the previous words, what is the next word.\n",
        "\n",
        "Our target classes are all the words in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BLXgGSH_aXd"
      },
      "outputs": [],
      "source": [
        "criterion = CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpeTMYfN_aXd"
      },
      "source": [
        "## Define training and evaluation functions\n",
        "- `train_epoch` trains the model for one epoch\n",
        "- `evaluate` evaluates the model on the validation set\n",
        "- `train_run` trains the model for all `EPOCHS` epochs and prints as it goes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJjFUtWg_aXd"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model: RecurrentModel, optimizer: torch.optim.Optimizer, train_data: Tensor, parameters:Parameters) -> Tuple[float,float]:\n",
        "    \"\"\"Training function for our recurrent model.\n",
        "\n",
        "    Args:\n",
        "        model (RecurrentModel): The model to train.\n",
        "        train_data (Tensor): Batchified training data.\n",
        "    \"\"\"\n",
        "    # Turn on training mode which enables training-specific\n",
        "    # layer functionality (e.g. dropout)\n",
        "    model.train()\n",
        "\n",
        "    # Keep track of the loss as we go\n",
        "    total_loss: float = 0.\n",
        "\n",
        "    # Determine how many steps we'll need to cover the whole dataset\n",
        "    num_batches: int = len(train_data) // parameters.BPTT\n",
        "\n",
        "    # Keep track of the hidden state between batches.\n",
        "    state = None\n",
        "\n",
        "    # For each of our batches\n",
        "    for i in range(0, train_data.size(0) - 1, parameters.BPTT):\n",
        "        # Throw away gradients from previous step\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get our input and target batches\n",
        "        data, targets = get_batch(train_data, i, parameters.BPTT)\n",
        "\n",
        "        # Evaluate the model to get logits and the new hidden state\n",
        "        output, state = model(data, state)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(output.view(-1, vocab_size), targets)\n",
        "\n",
        "        # Make sure the state does not carry gradients\n",
        "        # between batches so we avoid a runaway memory leak\n",
        "        model.detach_state_(state)\n",
        "\n",
        "        # Compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradients to avoid exploding gradients\n",
        "        # (Ask us TAs if you're curious why this is necessary)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "\n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Keep track of the loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Return the average loss across batches\n",
        "    cur_loss = total_loss / num_batches\n",
        "    # and the perplexity\n",
        "    ppl = math.exp(cur_loss)\n",
        "    return cur_loss, ppl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw9ePt8r_aXd"
      },
      "outputs": [],
      "source": [
        "def evaluate(model: RecurrentModel, val_data: Tensor, parameters: Parameters) -> Tuple[float, float]:\n",
        "    \"\"\"Evaluate the perplexity of the model on the validation data.\n",
        "\n",
        "    Args:\n",
        "        model (RecurrentModel): The model to evaluate.\n",
        "        val_data (Tensor): Batchified validation data.\n",
        "\n",
        "    Returns:\n",
        "        float: The perplexity of the model on the validation data.\n",
        "    \"\"\"\n",
        "    # Turn on evaluation mode which disables\n",
        "    # training-specific functionality (e.g. dropout)\n",
        "    model.eval()\n",
        "\n",
        "    # Keep track of the loss as we go\n",
        "    total_loss: float = 0.\n",
        "\n",
        "    # Determine how many steps we'll need to cover the whole dataset\n",
        "    num_batches: int = len(val_data) // parameters.BPTT\n",
        "\n",
        "    # Keep track of the hidden state between batches.\n",
        "    state = None\n",
        "    with torch.no_grad(): # No need to track gradients here, since we're not training\n",
        "        for i in range(0, val_data.size(0) - 1, parameters.BPTT):\n",
        "            # Get our input and target batches\n",
        "            data, targets = get_batch(val_data, i, parameters.BPTT)\n",
        "\n",
        "            # Evaluate the model to get logits and the new hidden state\n",
        "            output, state = model(data, state)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(output.view(-1, vocab_size), targets)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    # Return the average loss across batches\n",
        "    cur_loss = total_loss / num_batches\n",
        "    # and the perplexity\n",
        "    ppl = math.exp(cur_loss)\n",
        "    return cur_loss, ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0hLDXNj_aXd"
      },
      "outputs": [],
      "source": [
        "def train_run(model: RecurrentModel, train_data: Tensor, validation_data: Tensor, parameters: Parameters, verbose: bool = True) ->None:\n",
        "    \"\"\"Train a model for NUM_EPOCHS epochs.\n",
        "\n",
        "    Args:\n",
        "        model (RecurrentModel): The model to train.\n",
        "        train_data (Tensor): Batchified training data.\n",
        "    \"\"\"\n",
        "    # Here we define an optimizer and scheduler to use for training.\n",
        "    # An optimizer adjusts the parameters of the model based on the loss\n",
        "    # and the gradients of the parameters with respect to the loss.\n",
        "\n",
        "    # Adam is a popular optimizer that tends to work well in practice.\n",
        "    # optimizer = torch.optim.Adam(model.parameters(), lr=parameters.LR) # Set LR to about 0.005 to start\n",
        "\n",
        "    # AdamW corrects a minor flaw in the original Adam implementation\n",
        "    # allowing it to converge slightly better.\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=parameters.LR) # Set LR to about 0.01\n",
        "\n",
        "    # SGD is the simple stochastic gradient descent algorithm,\n",
        "    # implemented as LR * gradient.\n",
        "    # Notably, the RNNs shown here need a strong learning rate to\n",
        "    # even begin to move the loss, so we start with a high LR.\n",
        "    # optimizer = torch.optim.SGD(model.parameters(), lr=parameters.LR) # Set LR to about 10.0 to start\n",
        "\n",
        "    # The scheduler adjusts the learning rate over time.\n",
        "    # Here we use a scheduler that decreases the learning rate\n",
        "    # by a factor of 0.5 if the training loss doesn't decrease\n",
        "    # for two epochs in a row.\n",
        "    # This makes sure that the model can converge down into a\n",
        "    # minimum once it finds one, instead of bouncing around.\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=1, threshold=1e-3)\n",
        "\n",
        "    # Train for NUM_EPOCHS epochs\n",
        "    for epoch in range(1,parameters.EPOCHS+1,1):\n",
        "        if verbose:\n",
        "            # Print out an initial message stating the epoch number\n",
        "            # and learning rate\n",
        "            lrs = [group['lr'] for group in optimizer.param_groups]\n",
        "            lr = lrs[0] if len(lrs) > 0 else -1\n",
        "            # The end='' argument tells Python not to print a newline\n",
        "            # so we can extend this row with more information\n",
        "            # as we compute it.\n",
        "            print(f'| epoch {epoch:3d} | lr {lr:03.3f} | ', end='')\n",
        "\n",
        "        # Train for one epoch\n",
        "        loss, ppl = train_epoch(model, optimizer, train_data, parameters)\n",
        "\n",
        "        # Update the learning rate\n",
        "        scheduler.step(loss)\n",
        "        # Note that we can move this line after the validation step\n",
        "        # if we want to update the learning rate based on the val\n",
        "        # loss instead of the training loss.\n",
        "\n",
        "        if verbose:\n",
        "            # Update the printed row with the training loss and perplexity\n",
        "            # so that the user knows the training step completed and that\n",
        "            # we're now evaluating on the validation set.\n",
        "            print(f'train loss {loss:5.2f} | train ppl {ppl:8.2f} | ', end='')\n",
        "\n",
        "        # Evaluate on the validation set\n",
        "        val_loss, val_ppl = evaluate(model, validation_data, parameters)\n",
        "\n",
        "        if verbose:\n",
        "            # Update the printed row with the validation loss and perplexity\n",
        "            print(f'val loss {val_loss:5.2f} | val ppl {val_ppl:8.2f} |')\n",
        "            # We no longer have the end='' argument, so this completes the line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sm_NUJjg_aXe"
      },
      "outputs": [],
      "source": [
        "def create_model(\n",
        "    vocab_size: int,\n",
        "    parameters: Parameters,\n",
        "    model_type: Callable[..., Union[RNN, LSTM, GRU]],\n",
        ") -> RecurrentModel:\n",
        "    \"\"\"Create a recurrent model.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): The number of vocabulary in the dataset.\n",
        "        parameters (Parameters): The model parameters.\n",
        "        model_type (Callable[..., Union[RNN, LSTM, GRU]]): The type of recurrent\n",
        "            model to create.\n",
        "\n",
        "    Returns:\n",
        "        RecurrentModel: The recurrent model.\n",
        "    \"\"\"\n",
        "    # Create the model\n",
        "    # We've split the model into two pieces:\n",
        "    # 1. The RecurrentModel class, which defines the overall model architecture\n",
        "    #    including the embedding and output layer\n",
        "    # 2. The RNN, LSTM, or GRU class, which defines the recurrent unit\n",
        "    #    that will be used inside the model.\n",
        "    # This allows us to easily swap out different recurrent units\n",
        "    # without having to rewrite the whole model's code\n",
        "    # across multiple notebooks or cells.\n",
        "    return RecurrentModel(\n",
        "        vocab_size=vocab_size,\n",
        "        recurrent_module=model_type(\n",
        "            input_size=parameters.EMBEDDING_DIM,\n",
        "            hidden_size=parameters.HIDDEN_DIM,\n",
        "            num_layers=parameters.NUM_LAYERS,\n",
        "        ),\n",
        "    ).to(DEVICE)\n",
        "\n",
        "def train_model(\n",
        "    model: RecurrentModel,\n",
        "    unbatched_train_data: Tensor,\n",
        "    unbatched_validation_data: Tensor,\n",
        "    parameters: Parameters,\n",
        "    verbose: bool = True,\n",
        ") -> None:\n",
        "    if verbose:\n",
        "        print(f'The shape of the training data is {training_data.shape}.')\n",
        "    batched_training_data = batchify(unbatched_train_data, parameters.BATCH_SIZE)\n",
        "    if verbose:\n",
        "        print(f'The shape of the batched training data is {batched_training_data.shape}.')\n",
        "        print(f'The shape of the validation data is {validation_data.shape}.')\n",
        "    batched_validation_data = batchify(unbatched_validation_data, parameters.BATCH_SIZE)\n",
        "    if verbose:\n",
        "        print(f'The shape of the batched validation data is {batched_validation_data.shape}.')\n",
        "\n",
        "    train_run(model, batched_training_data, batched_validation_data, parameters, verbose)\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_and_train_model(\n",
        "    vocab_size: int,\n",
        "    parameters: Parameters,\n",
        "    model_type: Callable[..., Union[RNN, LSTM, GRU]],\n",
        "    unbatched_train_data: Tensor,\n",
        "    unbatched_validation_data: Tensor,\n",
        "    verbose: bool = True,\n",
        ") -> RecurrentModel:\n",
        "    \"\"\"Create and train a recurrent model.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): The number of vocabulary in the dataset.\n",
        "        parameters (Parameters): The model parameters.\n",
        "        model_type (Callable[..., Union[RNN, LSTM, GRU]]): The type of recurrent\n",
        "            model to create.\n",
        "        train_data (Tensor): Unbatched training data.\n",
        "\n",
        "    Returns:\n",
        "        RecurrentModel: The trained recurrent model.\n",
        "    \"\"\"\n",
        "    model = create_model(vocab_size, parameters, model_type)\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    train_model(model, unbatched_train_data, unbatched_validation_data, parameters, verbose)\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAWTbF3Z_aXe"
      },
      "source": [
        "## Run model-building experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XFY8W6l_aXe",
        "outputId": "89ecc0d2-c563-442a-9228-f8fa21ce4a2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape of the training data is torch.Size([1338875]).\n",
            "The shape of the batched training data is torch.Size([83679, 16]).\n",
            "The shape of the validation data is torch.Size([79221]).\n",
            "The shape of the batched validation data is torch.Size([4951, 16]).\n",
            "| epoch   1 | lr 0.010 | train loss  5.40 | train ppl   220.37 | val loss  5.83 | val ppl   341.90 |\n",
            "| epoch   2 | lr 0.010 | train loss  4.66 | train ppl   105.44 | val loss  5.72 | val ppl   305.23 |\n",
            "| epoch   3 | lr 0.010 | train loss  4.47 | train ppl    87.41 | val loss  5.64 | val ppl   282.11 |\n",
            "| epoch   4 | lr 0.010 | train loss  4.38 | train ppl    79.89 | val loss  5.66 | val ppl   288.24 |\n",
            "| epoch   5 | lr 0.010 | train loss  4.33 | train ppl    75.92 | val loss  5.66 | val ppl   286.63 |\n",
            "| epoch   6 | lr 0.010 | train loss  4.30 | train ppl    73.49 | val loss  5.63 | val ppl   278.41 |\n",
            "| epoch   7 | lr 0.010 | train loss  4.26 | train ppl    70.90 | val loss  5.61 | val ppl   272.99 |\n",
            "| epoch   8 | lr 0.010 | train loss  4.23 | train ppl    68.80 | val loss  5.52 | val ppl   249.45 |\n",
            "| epoch   9 | lr 0.010 | train loss  4.20 | train ppl    66.85 | val loss  5.55 | val ppl   258.14 |\n",
            "| epoch  10 | lr 0.010 | train loss  4.18 | train ppl    65.12 | val loss  5.60 | val ppl   269.43 |\n",
            "| epoch  11 | lr 0.010 | train loss  4.15 | train ppl    63.65 | val loss  5.61 | val ppl   273.09 |\n",
            "| epoch  12 | lr 0.010 | train loss  4.14 | train ppl    62.73 | val loss  5.58 | val ppl   265.54 |\n",
            "| epoch  13 | lr 0.010 | train loss  4.12 | train ppl    61.81 | val loss  5.58 | val ppl   265.71 |\n",
            "| epoch  14 | lr 0.010 | train loss  4.11 | train ppl    60.88 | val loss  5.61 | val ppl   274.33 |\n",
            "| epoch  15 | lr 0.010 | train loss  4.10 | train ppl    60.18 | val loss  5.55 | val ppl   257.65 |\n",
            "| epoch  16 | lr 0.010 | train loss  4.09 | train ppl    59.61 | val loss  5.49 | val ppl   242.18 |\n",
            "| epoch  17 | lr 0.010 | train loss  4.08 | train ppl    59.16 | val loss  5.54 | val ppl   254.78 |\n",
            "| epoch  18 | lr 0.010 | train loss  4.07 | train ppl    58.83 | val loss  5.54 | val ppl   255.04 |\n",
            "| epoch  19 | lr 0.010 | train loss  4.07 | train ppl    58.53 | val loss  5.53 | val ppl   253.22 |\n",
            "| epoch  20 | lr 0.010 | train loss  4.06 | train ppl    58.18 | val loss  5.49 | val ppl   242.06 |\n",
            "| epoch  21 | lr 0.010 | train loss  4.06 | train ppl    57.83 | val loss  5.50 | val ppl   243.94 |\n",
            "| epoch  22 | lr 0.010 | train loss  4.05 | train ppl    57.64 | val loss  5.52 | val ppl   249.44 |\n",
            "| epoch  23 | lr 0.010 | train loss  4.05 | train ppl    57.42 | val loss  5.53 | val ppl   252.03 |\n",
            "| epoch  24 | lr 0.010 | train loss  4.05 | train ppl    57.14 | val loss  5.47 | val ppl   236.63 |\n",
            "| epoch  25 | lr 0.010 | train loss  4.04 | train ppl    56.84 | val loss  5.49 | val ppl   242.02 |\n",
            "| epoch  26 | lr 0.010 | train loss  4.04 | train ppl    56.57 | val loss  5.49 | val ppl   242.28 |\n",
            "| epoch  27 | lr 0.010 | train loss  4.03 | train ppl    56.40 | val loss  5.47 | val ppl   236.38 |\n",
            "| epoch  28 | lr 0.010 | train loss  4.03 | train ppl    56.12 | val loss  5.51 | val ppl   247.09 |\n",
            "| epoch  29 | lr 0.010 | train loss  4.03 | train ppl    56.00 | val loss  5.56 | val ppl   259.81 |\n",
            "| epoch  30 | lr 0.010 | train loss  4.02 | train ppl    55.77 | val loss  5.54 | val ppl   253.76 |\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RecurrentModel(\n",
              "  (embedding): Embedding(19864, 16)\n",
              "  (rnn): GRU(16, 16, num_layers=3)\n",
              "  (linear): Linear(in_features=16, out_features=19864, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parameters1 = Parameters(\n",
        "    BATCH_SIZE=16,\n",
        "    BPTT=50,\n",
        "    LR=0.01,\n",
        "    EPOCHS=30,\n",
        "    EMBEDDING_DIM=16,\n",
        "    HIDDEN_DIM=16,\n",
        "    NUM_LAYERS=3,\n",
        ")\n",
        "model1 = create_model(\n",
        "    vocab_size=vocab_size,\n",
        "    parameters=parameters1,\n",
        "    model_type=GRU,\n",
        ")\n",
        "train_model(\n",
        "    model=model1,\n",
        "    unbatched_train_data=training_data,\n",
        "    unbatched_validation_data=validation_data,\n",
        "    parameters=parameters1,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_NFunQx_aXf"
      },
      "source": [
        "## Test model by generating text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoquMmLH_aXf"
      },
      "outputs": [],
      "source": [
        "def generate_text(\n",
        "    tokenizer: Tokenizer,\n",
        "    model: RecurrentModel,\n",
        "    input_text: str,\n",
        "    num_tokens_to_generate: int,\n",
        "    by_prob: bool = False,\n",
        ") -> str:\n",
        "    \"\"\"Input a text and use the model generate the next `n` words.\n",
        "\n",
        "    Args:\n",
        "        tokenizer (Tokenizer): The tokenizer used in training.\n",
        "        model (RNNModel): A trained recurrent neural network model.\n",
        "        text (str): A string of text to generate the text off of.\n",
        "        num_words (int, optional): The number of words to generate.\n",
        "            Defaults to 10.\n",
        "        by_prob (bool, optional): If `True`, words are randomly generated\n",
        "            based on probability. If `False`, words are generated based on\n",
        "            highest probability. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated text string.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set the model to evaluation mode (no Dropout and the like)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculation because we won't use it\n",
        "\n",
        "        # Process our input text\n",
        "        data = data_process([input_text], tokenizer)\n",
        "        # Turn it into a batch for the model\n",
        "        x = data.reshape((-1, 1)).to(DEVICE)\n",
        "\n",
        "        state = None # Start with a blank state\n",
        "        output_tokens = [] # Keep track of the generated tokens\n",
        "        for _ in range(num_tokens_to_generate):\n",
        "            # Get the model's predicted next token\n",
        "            y_pred, state = model(x, state)\n",
        "            # We only care about the last token's predictions\n",
        "            last_word_logits = y_pred[-1][0]\n",
        "\n",
        "            # If we're generating by probability, we'll randomly choose a word\n",
        "            if by_prob:\n",
        "                # Normalize the logits so they form a probability distribution\n",
        "                p = (\n",
        "                    torch.nn.functional.softmax(last_word_logits, dim=0)\n",
        "                    .cpu()\n",
        "                    .numpy()\n",
        "                )\n",
        "                # Randomly choose the next word based on the probability\n",
        "                word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "            else:\n",
        "                # Otherwise, we'll just choose the word with the highest probability\n",
        "                word_index = torch.argmax(last_word_logits)\n",
        "\n",
        "            # Add the generated word to our output\n",
        "            output_tokens.append(word_index)\n",
        "\n",
        "            # Add the generated word as the next input to the model\n",
        "            x = torch.tensor([word_index]).reshape(1, 1).to(DEVICE)\n",
        "\n",
        "    # Decode the generated tokens and return the result\n",
        "    return tokenizer.decode(output_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfvJ1l7v_aXf",
        "outputId": "f84ff980-5505-4275-aebb-7fcd29f11a40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a senator, and others \n",
            " king henry v :\n"
          ]
        }
      ],
      "source": [
        "print(generate_text(\n",
        "    tokenizer,\n",
        "    model1,\n",
        "    \"To be or not to be\",\n",
        "    10,\n",
        "    by_prob=False,\n",
        "))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}